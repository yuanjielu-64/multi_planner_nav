# RLFT (Reinforcement Learning Fine-Tuning)

VLM+DPT ä¸ TD3 ç»“åˆçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒå®ç°

## ğŸ“ ç›®å½•ç»“æ„

```
rlft/
â”œâ”€â”€ __init__.py          # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ vlm_net.py           # VLM+DPTç½‘ç»œå®šä¹‰
â”œâ”€â”€ rl.py                # TD3ç®—æ³•å®ç° (å¤ç”¨è‡ªtd3/)
â”œâ”€â”€ train.py             # FTRLè®­ç»ƒè„šæœ¬
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. VLM_DPT_FeatureExtractor
ä»ç›‘ç£å­¦ä¹ checkpointåŠ è½½VLM+DPTä½œä¸ºç‰¹å¾æå–å™¨

**å…³é”®è®¾è®¡**:
- åŠ è½½é¢„è®­ç»ƒçš„Qwen2.5-VL + LoRA
- åŠ è½½é¢„è®­ç»ƒçš„DPT Head
- æ”¯æŒé€‰æ‹©æ€§å†»ç»“ï¼ˆVLM/DPTç‹¬ç«‹æ§åˆ¶ï¼‰
- åªæå–DPTçš„ä¸­é—´ç‰¹å¾ï¼ˆ256-dï¼‰ï¼Œä¸ä½¿ç”¨å›å½’å±‚

### 2. VLM_DPT_Actor
TD3çš„Actorç½‘ç»œ

**æ¶æ„**:
```
Costmap Image â†’ VLM+DPTç‰¹å¾æå– â†’ FCå±‚ â†’ 7ä¸ªå¯¼èˆªå‚æ•°
```

**è®­ç»ƒç­–ç•¥**:
- VLM: å†»ç»“ï¼ˆå¤ªå¤§ï¼Œæ›´æ–°æ…¢ï¼‰
- DPT Head: å¯è®­ç»ƒï¼ˆFTRLå¾®è°ƒï¼‰
- FCå±‚: å¯è®­ç»ƒ

### 3. VLM_DPT_Critic
TD3çš„Twin Criticç½‘ç»œ

**æ¶æ„**:
```
Costmap Image â†’ VLM+DPTç‰¹å¾æå– â†’ [256-d]
                                    â†“
Action (7ä¸ªå‚æ•°) â†’ MLPç¼–ç  â†’ [64-d] â†“
                                    â†“
            Concat â†’ Fusion MLP â†’ Qå€¼
```

**è®­ç»ƒç­–ç•¥**:
- VLM+DPT: å…¨éƒ¨å†»ç»“ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
- Actionç¼–ç å™¨: å¯è®­ç»ƒ
- Fusion MLP: å¯è®­ç»ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡checkpoint

ç¡®ä¿ä½ æœ‰ç›‘ç£å­¦ä¹ è®­ç»ƒå¥½çš„VLM+DPT checkpointï¼š

```bash
checkpoint-2500/
â”œâ”€â”€ adapter_config.json      # LoRAé…ç½®
â”œâ”€â”€ adapter_model.bin         # LoRAæƒé‡
â””â”€â”€ regression_head/
    â””â”€â”€ pytorch_model.bin     # DPT Headæƒé‡
```

### 2. ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `script/ft_qwen/configs/ftrl_vlm_dwa.yaml`:

```yaml
training_config:
  # ä¿®æ”¹ä¸ºä½ çš„checkpointè·¯å¾„
  vlm_checkpoint_path: "/path/to/your/checkpoint-2500"
```

### 3. å¯åŠ¨è®­ç»ƒ

**æ–¹å¼1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰**
```bash
cd /path/to/ros_jackal
./script/ft_qwen/run_ftrl.sh
```

**æ–¹å¼2: ç›´æ¥è¿è¡ŒPython**
```bash
cd /path/to/ros_jackal
python rlft/train.py \
  --config_path script/ft_qwen/configs/ \
  --config_file ftrl_vlm_dwa \
  --buffer_path buffer/ftrl_vlm \
  --logging_path logging/ftrl_vlm
```

### 4. ç›‘æ§è®­ç»ƒ

ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼š

```bash
tensorboard --logdir logging/ftrl_vlm
```

**å…³é”®æŒ‡æ ‡**:
- `train/Test_nav_metric`: æµ‹è¯•é›†å¯¼èˆªæ€§èƒ½ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- `train/Success_rate`: æˆåŠŸç‡
- `train/Actor_loss`: ActoræŸå¤±
- `train/Critic_loss`: CriticæŸå¤±

## âš™ï¸ é…ç½®è¯´æ˜

### VLM+DPTé…ç½®

```yaml
training_config:
  # Checkpointè·¯å¾„
  vlm_checkpoint_path: "/path/to/checkpoint"

  # å†»ç»“ç­–ç•¥
  freeze_vlm_actor: true     # Actorçš„VLMå†»ç»“ï¼ˆæ¨èï¼‰
  freeze_dpt_actor: false    # Actorçš„DPTå¯è®­ç»ƒï¼ˆFTRLï¼‰
  freeze_dpt_critic: true    # Criticçš„DPTå†»ç»“ï¼ˆçœæ˜¾å­˜ï¼‰
```

### å­¦ä¹ ç‡é…ç½®

```yaml
training_config:
  # VLMå¾®è°ƒéœ€è¦æ›´å°çš„å­¦ä¹ ç‡
  actor_lr: 1.0e-5    # æ¯”APPLRå°10å€
  critic_lr: 3.0e-4
```

### TD3è¶…å‚æ•°

```yaml
training_config:
  policy_args:
    gamma: 0.99              # æŠ˜æ‰£å› å­
    tau: 0.005               # è½¯æ›´æ–°ç³»æ•°
    policy_noise: 0.2        # ç›®æ ‡ç­–ç•¥å¹³æ»‘å™ªå£°
    noise_clip: 0.5          # å™ªå£°è£å‰ª
    n_step: 4                # N-step return
    update_actor_freq: 2     # Actorå»¶è¿Ÿæ›´æ–°
    exploration_noise: 0.1   # æ¢ç´¢å™ªå£°
```

### è®­ç»ƒå‚æ•°

```yaml
training_config:
  training_args:
    max_step: 1000000         # æ€»è®­ç»ƒæ­¥æ•°
    collect_per_step: 1000    # æ¯æ¬¡æ”¶é›†æ­¥æ•°
    update_per_step: 50       # æ¯æ¬¡æ›´æ–°æ¬¡æ•°
    batch_size: 256           # æ‰¹å¤§å°
```

## ğŸ¯ ä¸APPLRçš„åŒºåˆ«

| æ–¹é¢ | APPLR (Baseline) | RLFT (æœ¬å®ç°) |
|------|------------------|---------------|
| ç‰¹å¾æå– | CNN (3å±‚Conv) | VLM+DPT (é¢„è®­ç»ƒ) |
| åˆå§‹åŒ– | éšæœºåˆå§‹åŒ– | ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ |
| è®­ç»ƒæ•°æ®éœ€æ±‚ | 5M samples | é¢„æœŸ<1M samples |
| Actorå‚æ•°é‡ | ~1M | ~8B (å¤§éƒ¨åˆ†å†»ç»“) |
| æ ·æœ¬æ•ˆç‡ | ä½ | é«˜ï¼ˆé¢„è®­ç»ƒåŠ æŒï¼‰ |
| è®­ç»ƒæ—¶é—´ | 6å°æ—¶ (500 CPU) | å¾…æµ‹è¯• |

## ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹

### 1. Criticä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥å¤ç”¨Actorçš„checkpointï¼Ÿ

**é—®é¢˜**: Actorå’ŒCriticçš„è¾“å…¥ç©ºé—´ä¸åŒ
- Actor: `state` â†’ `action`
- Critic: `(state, action)` â†’ `Qå€¼`

**è§£å†³**: Criticä½¿ç”¨VLM+DPTæå–stateç‰¹å¾ï¼Œé¢å¤–ç”¨MLPç¼–ç actionï¼Œç„¶åfusion

### 2. ä¸ºä»€ä¹ˆè¦å†»ç»“VLMï¼Ÿ

**åŸå› **:
- VLMæœ‰8Bå‚æ•°ï¼ŒRLæ›´æ–°å¤ªæ…¢
- VLMçš„è§†è§‰ç†è§£èƒ½åŠ›å·²ç»å¾ˆå¼ºï¼Œä¸éœ€è¦ç»§ç»­è®­ç»ƒ
- èŠ‚çœæ˜¾å­˜å’Œè®¡ç®—

**FTRLç­–ç•¥**: åªå¾®è°ƒDPT Headï¼ˆ256-dç‰¹å¾ç©ºé—´çš„å›å½’ï¼‰

### 3. ä¸ºä»€ä¹ˆCriticçš„DPTä¹Ÿå†»ç»“ï¼Ÿ

**åŸå› **:
- Criticä¸éœ€è¦ç›´æ¥é¢„æµ‹å‚æ•°ï¼Œåªéœ€è¦è¯„ä¼°å¥½å
- å†»ç»“DPTå¯ä»¥èŠ‚çœå¤§é‡æ˜¾å­˜ï¼ˆåŒQç½‘ç»œéœ€è¦2ä¸ªVLMï¼‰
- Criticçš„fusionå±‚å·²ç»è¶³å¤Ÿå­¦ä¹ Qå€¼

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch_sizeï¼ˆ256 â†’ 128 â†’ 64ï¼‰
2. ä½¿ç”¨4-bité‡åŒ–åŠ è½½VLM
3. åªåœ¨Actorä¸­ä½¿ç”¨VLMï¼ŒCriticç”¨è½»é‡CNN

### Q2: è®­ç»ƒä¸ç¨³å®šï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°actor_lrï¼ˆ1e-5 â†’ 5e-6ï¼‰
2. å¢åŠ pre_collectï¼ˆ10000 â†’ 50000ï¼‰
3. å‡å°exploration_noise_startï¼ˆ0.05 â†’ 0.02ï¼‰

### Q3: VLMåŠ è½½å¤±è´¥ï¼Ÿ

**æ£€æŸ¥**:
1. checkpointè·¯å¾„æ˜¯å¦æ­£ç¡®
2. æ˜¯å¦åŒ…å«`adapter_config.json`ï¼ˆLoRAï¼‰
3. æ˜¯å¦åŒ…å«`regression_head/pytorch_model.bin`ï¼ˆDPTï¼‰

### Q4: æ€§èƒ½ä¸å¦‚ç›‘ç£å­¦ä¹ ï¼Ÿ

**å¯èƒ½åŸå› **:
1. RLæ¢ç´¢ç ´åäº†é¢„è®­ç»ƒçŸ¥è¯† â†’ å‡å°exploration noise
2. å­¦ä¹ ç‡å¤ªå¤§ â†’ å‡å°actor_lr
3. è®­ç»ƒæ­¥æ•°ä¸å¤Ÿ â†’ å¢åŠ max_step

## ğŸ“Š é¢„æœŸæ€§èƒ½

**ç›‘ç£å­¦ä¹ baseline**:
- MAE: ~0.05-0.1ï¼ˆå½’ä¸€åŒ–åï¼‰
- æ¨ç†é€Ÿåº¦: ~100-500ms/frame

**FTRLç›®æ ‡**:
- å¯¼èˆªæˆåŠŸç‡: è¶…è¿‡ç›‘ç£å­¦ä¹ 
- æ ·æœ¬æ•ˆç‡: <1M stepsï¼ˆvs APPLRçš„5Mï¼‰
- è®­ç»ƒæ—¶é—´: ~24å°æ—¶ï¼ˆå•GPUï¼‰

## ğŸ”¬ å®éªŒå»ºè®®

### æ¶ˆèå®éªŒ

1. **VLMå†»ç»“vså¾®è°ƒ**
   - é…ç½®: `freeze_vlm_actor: true/false`
   - å¯¹æ¯”è®­ç»ƒé€Ÿåº¦å’Œæ€§èƒ½

2. **DPTå†»ç»“vså¾®è°ƒ**
   - é…ç½®: `freeze_dpt_actor: true/false`
   - éªŒè¯FTRLçš„å¿…è¦æ€§

3. **ä¸åŒå­¦ä¹ ç‡**
   - `actor_lr: [1e-6, 5e-6, 1e-5, 5e-5]`
   - æ‰¾æœ€ä¼˜å­¦ä¹ ç‡

### å¯¹æ¯”å®éªŒ

1. **FTRL vs ç›‘ç£å­¦ä¹ **
   - åœ¨ç›¸åŒæµ‹è¯•ç¯å¢ƒè¯„ä¼°
   - å¯¹æ¯”æˆåŠŸç‡ã€è½¨è¿¹å¹³æ»‘åº¦

2. **FTRL vs APPLR**
   - æ ·æœ¬æ•ˆç‡å¯¹æ¯”
   - æ€§èƒ½ä¸Šç•Œå¯¹æ¯”

## ğŸ“š å‚è€ƒæ–‡çŒ®

- APPLR: Adaptive Planner Parameter Learning from Reinforcement
- TD3: Twin Delayed Deep Deterministic Policy Gradient
- DPT: Dense Prediction Transformer (å‚è€ƒDUSt3R)
- Qwen2.5-VL: è§†è§‰è¯­è¨€æ¨¡å‹

## ğŸ¤ è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–æ”¹è¿›å»ºè®®ï¼Œè¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

---

**Happy Fine-Tuning! ğŸš€**
