import warnings
# æŠ‘åˆ¶PEFTçš„å†—é•¿è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='peft')

import torch
import torch.nn as nn
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from peft import PeftModel
import sys
import os

# æ·»åŠ qwen_dptæ¨¡å‹è·¯å¾„
qwen_dpt_path = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    "qwen_dpt/lmms-finetune-qwen/models"
)
sys.path.append(qwen_dpt_path)

try:
    from qwen2_5_vl_dpt_regression import DPTHead, HistoryEncoder
except ImportError:
    print(f"Warning: Cannot import from qwen2_5_vl_dpt_regression")
    print(f"Expected path: {qwen_dpt_path}")


class VLM_DPT_FeatureExtractor(nn.Module):
    """
    VLM+DPTç‰¹å¾æå–å™¨ - ä»checkpointåŠ è½½

    ç”¨äºActorå’ŒCriticå…±äº«è§†è§‰ç‰¹å¾æå–

    Args:
        checkpoint_path: VLM+DPTç›‘ç£å­¦ä¹ checkpointè·¯å¾„
        freeze_vlm: æ˜¯å¦å†»ç»“VLMå‚æ•° (æ¨èTrueï¼Œå› ä¸ºVLMå¤ªå¤§)
        freeze_dpt: æ˜¯å¦å†»ç»“DPT head (Actorå»ºè®®Falseè¿›è¡ŒFTRLå¾®è°ƒ)
        freeze_history: æ˜¯å¦å†»ç»“History Encoder (å¦‚æœä½¿ç”¨å†å²å¸§)
        device: è®¾å¤‡
        use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ– (æ¨èTrueèŠ‚çœæ˜¾å­˜)
    """
    def __init__(
        self,
        checkpoint_path,
        freeze_vlm=True,
        freeze_dpt=False,
        freeze_history=None,  # Noneè¡¨ç¤ºè·Ÿéšfreeze_dptï¼Œä¹Ÿå¯ä»¥ç‹¬ç«‹è®¾ç½®
        device="cuda",
        use_4bit=True,
        algorithm="DWA"  # ç®—æ³•ç±»å‹ (DWA/TEB/MPPI/DDP)
    ):
        super().__init__()

        self.checkpoint_path = checkpoint_path
        self.device = device
        self.algorithm = algorithm.upper()  # å­˜å‚¨ç®—æ³•ç±»å‹

        # 1. åŠ è½½base VLM (ä½¿ç”¨4-bité‡åŒ–)
        # Qwen2.5-VL-3B çš„ hidden_size æ˜¯ 2048ï¼ˆä¸7Bç›¸åŒï¼‰
        print(f"[VLM_DPT_FeatureExtractor] Loading VLM from Qwen/Qwen2.5-VL-3B-Instruct...")

        if use_4bit and "cuda" in device:
            print("[VLM_DPT_FeatureExtractor] Using 4-bit quantization to save memory")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "Qwen/Qwen2.5-VL-3B-Instruct",
                quantization_config=bnb_config,
                device_map=device
            )
        else:
            self.base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "Qwen/Qwen2.5-VL-3B-Instruct",
                torch_dtype=torch.bfloat16,
                device_map=device
            )

        # ç»Ÿè®¡base VLMå‚æ•°
        base_vlm_params = sum(p.numel() for p in self.base_model.parameters())
        print(f"[VLM_DPT_FeatureExtractor] âœ“ Base VLM loaded: {base_vlm_params:,} parameters ({base_vlm_params/1e9:.2f}B)")

        # 2. åŠ è½½LoRA (å¦‚æœcheckpointåŒ…å«LoRA)
        lora_path = checkpoint_path
        if os.path.exists(os.path.join(checkpoint_path, "adapter_config.json")):
            print(f"[VLM_DPT_FeatureExtractor] Loading LoRA from {lora_path}...")

            # æ£€æŸ¥ adapter_model.safetensors æ˜¯å¦ä¸ºç©º
            adapter_weights_path = os.path.join(lora_path, "adapter_model.safetensors")
            if os.path.exists(adapter_weights_path):
                from safetensors.torch import load_file
                adapter_state = load_file(adapter_weights_path)
                non_empty_params = {k: v for k, v in adapter_state.items() if v.numel() > 0}

                if len(non_empty_params) == 0:
                    print(f"âš ï¸  WARNING: adapter_model.safetensors is EMPTY (0 parameters)!")
                    print(f"âš ï¸  LoRA will be randomly initialized (not using trained weights)")
                    print(f"âš ï¸  This is OK for current testing, but will need to be fixed for full training")
                else:
                    total_lora_params = sum(v.numel() for v in non_empty_params.values())
                    print(f"âœ“ LoRA adapter contains {len(non_empty_params)} keys, {total_lora_params:,} parameters ({total_lora_params/1e6:.2f}M)")

            self.base_model = PeftModel.from_pretrained(
                self.base_model,
                lora_path,
                is_trainable=(not freeze_vlm)  # å¦‚æœVLMå¯è®­ç»ƒï¼Œä¿æŒLoRAå¯è®­ç»ƒ
            )

            # å…³é”®å†³ç­–ï¼šæ˜¯å¦merge LoRA
            if freeze_vlm:
                # VLMå†»ç»“ï¼šmerge LoRAä»¥èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿæ¨ç†
                self.base_model = self.base_model.merge_and_unload()
                print(f"[VLM_DPT_FeatureExtractor] âœ“ LoRA merged into base model (VLM frozen)")
            else:
                # VLMå¯è®­ç»ƒï¼šä¿æŒLoRAä½œä¸ºç‹¬ç«‹å±‚ï¼Œä»¥ä¾¿è®­ç»ƒå’Œä¿å­˜
                lora_params = sum(p.numel() for n, p in self.base_model.named_parameters() if 'lora' in n.lower())
                print(f"[VLM_DPT_FeatureExtractor] âœ“ LoRA loaded as trainable layers: {lora_params:,} parameters ({lora_params/1e6:.2f}M)")
        else:
            print(f"[VLM_DPT_FeatureExtractor] No LoRA found, using base VLM")

        # 3. åŠ è½½DPT head
        regression_head_path = os.path.join(checkpoint_path, "regression_head", "pytorch_model.bin")
        if not os.path.exists(regression_head_path):
            # Fallback: å°è¯•ä¸å¸¦å­ç›®å½•çš„è·¯å¾„
            regression_head_path = os.path.join(checkpoint_path, "pytorch_model.bin")

        # 3.1 è¯»å–å†å²å¸§é…ç½®
        history_config_path = os.path.join(checkpoint_path, "history_config.json")
        if os.path.exists(history_config_path):
            import json
            with open(history_config_path) as f:
                history_config = json.load(f)
                self.use_history = history_config.get('use_history', False)
                self.num_history_frames = history_config.get('num_history_frames', 2)
                self.history_dim = history_config.get('history_dim', 256)
            print(f"[VLM_DPT_FeatureExtractor] History config: use_history={self.use_history}, num_frames={self.num_history_frames}")
        else:
            self.use_history = False
            self.num_history_frames = 2
            self.history_dim = 256
            print(f"[VLM_DPT_FeatureExtractor] No history_config.json found, using use_history=False")

        # 3.2 åˆ›å»ºDPT Head
        print(f"[VLM_DPT_FeatureExtractor] Loading DPT head from {regression_head_path}...")
        self.dpt_head = DPTHead(
            hidden_size=2048,  # Qwen2.5-VL-3B (ä¸checkpointåŒ¹é…)
            num_params=8,      # DDPæœ‰8ä¸ªå‚æ•°
            feature_dim=256,
            num_layers=4,
            use_history=self.use_history,  # ä»é…ç½®è¯»å–
            history_dim=self.history_dim
        )

        if os.path.exists(regression_head_path):
            state_dict = torch.load(regression_head_path, map_location='cpu')
            # ç»Ÿè®¡åŠ è½½çš„å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in state_dict.values() if p.numel() > 0)
            num_keys = len([k for k, v in state_dict.items() if v.numel() > 0])

            self.dpt_head.load_state_dict(state_dict, strict=False)
            print(f"[VLM_DPT_FeatureExtractor] âœ“ DPT head loaded: {num_keys} keys, {total_params:,} parameters ({total_params/1e6:.2f}M)")
        else:
            print(f"[VLM_DPT_FeatureExtractor] âš ï¸  Warning: DPT head not found at {regression_head_path}")
            print("[VLM_DPT_FeatureExtractor] Using randomly initialized DPT head")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šDPT head ä¿æŒ float32ï¼Œä¸è¦è½¬ bfloat16ï¼
        # åŸå› ï¼šbfloat16 ç²¾åº¦å¤ªä½ï¼Œåœ¨ TD3 è®­ç»ƒçš„æ¢¯åº¦æ›´æ–°ä¸­å®¹æ˜“å¯¼è‡´ NaN
        # VLM è¾“å‡ºçš„ bfloat16 hidden states ä¼šåœ¨ forward ä¸­è‡ªåŠ¨è½¬æ¢ä¸º float32
        self.dpt_head.to(device=device, dtype=torch.float32)
        print(f"[VLM_DPT_FeatureExtractor] âœ“ DPT head moved to device={device}, dtype=float32 (NOT bfloat16 for stability)")

        # 3.3 åŠ è½½History Encoder (å¦‚æœä½¿ç”¨)
        self.history_encoder = None
        if self.use_history:
            # ç›‘ç£å­¦ä¹ ä¿å­˜æ ¼å¼: history_encoder/pytorch_model.bin
            history_encoder_path = os.path.join(checkpoint_path, "history_encoder", "pytorch_model.bin")

            # Fallback: å°è¯•æ—§æ ¼å¼ history_encoder.pth
            if not os.path.exists(history_encoder_path):
                history_encoder_path = os.path.join(checkpoint_path, "history_encoder.pth")

            if os.path.exists(history_encoder_path):
                self.history_encoder = HistoryEncoder(
                    hidden_dim=self.history_dim,
                    num_frames=self.num_history_frames,
                    num_transformer_layers=2
                )
                history_state_dict = torch.load(history_encoder_path, map_location='cpu')

                # ç»Ÿè®¡å‚æ•°æ•°é‡
                total_history_params = sum(p.numel() for p in history_state_dict.values() if p.numel() > 0)
                num_history_keys = len([k for k, v in history_state_dict.items() if v.numel() > 0])

                self.history_encoder.load_state_dict(history_state_dict)
                # ğŸ”§ å…³é”®ä¿®å¤: History encoder ä¿æŒ float32ï¼Œä¸è¦è½¬ bfloat16
                # CNN + Transformer åœ¨ bfloat16 ä¸‹æ•°å€¼ä¸ç¨³å®šï¼Œå®¹æ˜“äº§ç”Ÿ NaN
                self.history_encoder.to(device=device, dtype=torch.float32)
                print(f"[VLM_DPT_FeatureExtractor] âœ“ History encoder loaded: {num_history_keys} keys, {total_history_params:,} parameters ({total_history_params/1e6:.2f}M)")
                print(f"[VLM_DPT_FeatureExtractor] âœ“ History encoder moved to device={device}, dtype=float32 (NOT bfloat16 for stability)")
            else:
                print(f"[VLM_DPT_FeatureExtractor] âš ï¸  Warning: use_history=True but history encoder not found")
                print(f"[VLM_DPT_FeatureExtractor] Expected: {os.path.join(checkpoint_path, 'history_encoder/pytorch_model.bin')}")
                self.use_history = False  # é™çº§åˆ°ä¸ä½¿ç”¨å†å²å¸§

        # 4. å†»ç»“ç­–ç•¥
        if freeze_vlm:
            for param in self.base_model.parameters():
                param.requires_grad = False
            print("[VLM_DPT_FeatureExtractor] âœ“ VLM frozen")
        else:
            print("[VLM_DPT_FeatureExtractor] VLM trainable")

        if freeze_dpt:
            for param in self.dpt_head.parameters():
                param.requires_grad = False
            print("[VLM_DPT_FeatureExtractor] âœ“ DPT head frozen")
        else:
            print("[VLM_DPT_FeatureExtractor] DPT head trainable")

        # History encoderç‹¬ç«‹å†»ç»“ç­–ç•¥
        if self.history_encoder is not None:
            # å¦‚æœfreeze_historyæœªæŒ‡å®šï¼Œè·Ÿéšfreeze_dpt
            freeze_history_final = freeze_history if freeze_history is not None else freeze_dpt

            if freeze_history_final:
                for param in self.history_encoder.parameters():
                    param.requires_grad = False
                print("[VLM_DPT_FeatureExtractor] âœ“ History encoder frozen")
            else:
                print("[VLM_DPT_FeatureExtractor] History encoder trainable")

        self.feature_dim = 256  # DPTè¾“å‡ºç»´åº¦

        # 5. åŠ è½½processor (ç”¨äºå›¾åƒé¢„å¤„ç†)
        self.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2.5-VL-3B-Instruct",
            use_fast=True  # ä½¿ç”¨å¿«é€Ÿå¤„ç†å™¨ï¼Œé¿å…è­¦å‘Š
        )

        # 6. å‚æ•°ç»Ÿè®¡æ±‡æ€»
        print("\n" + "="*70)
        print("ğŸ“Š VLM_DPT_FeatureExtractor Parameter Summary")
        print("="*70)

        # VLM Base Model
        vlm_total = sum(p.numel() for p in self.base_model.parameters())
        vlm_trainable = sum(p.numel() for p in self.base_model.parameters() if p.requires_grad)
        print(f"VLM Base (Qwen2.5-VL-3B):")
        print(f"  â”œâ”€ Total:      {vlm_total:>12,} parameters ({vlm_total/1e9:.2f}B)")
        print(f"  â”œâ”€ Trainable:  {vlm_trainable:>12,} parameters ({vlm_trainable/1e6:.2f}M)")
        print(f"  â””â”€ Frozen:     {vlm_total-vlm_trainable:>12,} parameters ({(vlm_total-vlm_trainable)/1e9:.2f}B)")

        # DPT Head
        dpt_total = sum(p.numel() for p in self.dpt_head.parameters())
        dpt_trainable = sum(p.numel() for p in self.dpt_head.parameters() if p.requires_grad)
        print(f"\nDPT Head:")
        print(f"  â”œâ”€ Total:      {dpt_total:>12,} parameters ({dpt_total/1e6:.2f}M)")
        print(f"  â”œâ”€ Trainable:  {dpt_trainable:>12,} parameters ({dpt_trainable/1e6:.2f}M)")
        print(f"  â””â”€ Frozen:     {dpt_total-dpt_trainable:>12,} parameters ({(dpt_total-dpt_trainable)/1e6:.2f}M)")

        # History Encoder (å¦‚æœå­˜åœ¨)
        if self.history_encoder is not None:
            hist_total = sum(p.numel() for p in self.history_encoder.parameters())
            hist_trainable = sum(p.numel() for p in self.history_encoder.parameters() if p.requires_grad)
            print(f"\nHistory Encoder:")
            print(f"  â”œâ”€ Total:      {hist_total:>12,} parameters ({hist_total/1e6:.2f}M)")
            print(f"  â”œâ”€ Trainable:  {hist_trainable:>12,} parameters ({hist_trainable/1e6:.2f}M)")
            print(f"  â””â”€ Frozen:     {hist_total-hist_trainable:>12,} parameters ({(hist_total-hist_trainable)/1e6:.2f}M)")
        else:
            print(f"\nHistory Encoder: Not loaded")

        # æ€»è®¡
        total_params = vlm_total + dpt_total
        total_trainable = vlm_trainable + dpt_trainable
        if self.history_encoder is not None:
            total_params += hist_total
            total_trainable += hist_trainable

        print(f"\n{'â”€'*70}")
        print(f"Total Feature Extractor:")
        print(f"  â”œâ”€ Total:      {total_params:>12,} parameters ({total_params/1e9:.2f}B)")
        print(f"  â”œâ”€ Trainable:  {total_trainable:>12,} parameters ({total_trainable/1e6:.2f}M)")
        print(f"  â”œâ”€ Frozen:     {total_params-total_trainable:>12,} parameters ({(total_params-total_trainable)/1e9:.2f}B)")
        print(f"  â””â”€ Trainable%: {100*total_trainable/total_params:>11.2f}%")
        print("="*70 + "\n")

    def forward(self, images, prompt=None, history_images=None,
                linear_vels=None, angular_vels=None, algorithm="DWA"):
        """
        Args:
            images: PIL.Image list æˆ– [B, 3, H, W] tensor (å½“å‰å¸§)
            prompt: str, å¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤å¯¼èˆªprompt
            history_images: [B, num_frames, 3, H, W] tensor æˆ– List[List[PIL.Image]] (å†å²å¸§ï¼Œå¯é€‰)
            linear_vels: List[float] (å½“å‰çº¿é€Ÿåº¦ï¼Œå¯é€‰ï¼Œç”¨äºç”Ÿæˆprompt)
            angular_vels: List[float] (å½“å‰è§’é€Ÿåº¦ï¼Œå¯é€‰ï¼Œç”¨äºç”Ÿæˆprompt)
            algorithm: str (è§„åˆ’ç®—æ³•åç§°ï¼Œé»˜è®¤DWA)

        Returns:
            features: [B, 256] ç‰¹å¾å‘é‡
        """
        # System prompt (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        SYSTEM_PROMPT = """You are a navigation scene analyzer for Clearpath Jackal robot motion planning.
Scene Representation (Costmap Visualization):
- Scale: Each grid cell represents approximately 1 meter.
- Robot: Yellow square (0.508 m x 0.430 m footprint)
- Green arrow: forward driving direction (x-axis)
- Blue line: lateral direction (y-axis)
- Obstacles: Red points from laser scanner measurements
- Global Path: Black curve showing a kinematic reference trajectory that guides direction but does not encode dynamics or feasibility.
- Background: Grey grid is for visualization only and has no obstacle or cost meaning.
Your Role:
Analyze the spatial layout to understand:
- Obstacle distribution and density around the robot
- Available free space and safe clearance margins
- Path geometry (straight corridor, sharp turn, narrow passage)
- Motion feasibility given the current velocity state

Your navigation reasoning will be used downstream to select parameters for the local motion planner."""

        # æ„å»ºå®Œæ•´çš„chat messages
        if not isinstance(images, list):
            # Tensor -> æš‚æ—¶ä¸æ”¯æŒï¼Œå› ä¸ºchat templateéœ€è¦PIL.Image
            raise ValueError("QwenVLMFeatureExtractor only supports PIL.Image list in forward()")

        batch_size = len(images)
        texts = []

        for i in range(batch_size):
            # User prompt (åŒ…å«é€Ÿåº¦ä¿¡æ¯)
            if linear_vels is not None and angular_vels is not None:
                user_prompt = f"""Current robot state:
- Linear velocity: {linear_vels[i]:.3f} m/s
- Angular velocity: {angular_vels[i]:.3f} rad/s

Target local planner: {algorithm}

Use the scene image and robot state to perform navigation reasoning about obstacle proximity, path curvature, free-space structure, and motion constraints."""
            else:
                user_prompt = f"""Target local planner: {algorithm}

Analyze the costmap to understand obstacle distribution and path geometry."""

            # æ„å»ºmessages (ä¸qwen_server_flash_attn.pyä¸€è‡´)
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT.strip()},
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": images[i]},
                        {"type": "text", "text": user_prompt.strip()},
                    ],
                }
            ]

            # åº”ç”¨chat template
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False  # å›å½’ä»»åŠ¡ä¸ç”Ÿæˆassistantå›å¤
            )
            texts.append(text)

        # Processorå¤„ç† (ä¸qwen_server_flash_attn.pyä¸€è‡´)
        inputs = self.processor(
            text=texts,
            images=images,
            videos=None,
            padding=False,  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
            return_tensors="pt"
        ).to(self.device)

        # VLMå‰å‘ (è·å–hidden states)
        # ğŸ”§ å…³é”®ï¼šä½¿ç”¨ self.base_model(**inputs) è€Œä¸æ˜¯ self.base_model.model(**inputs)
        # - base_model (Qwen2_5_VLForConditionalGeneration) ä¼šå…ˆé€šè¿‡ vision encoder å¤„ç† pixel_values
        # - base_model.model (Qwen2_5_VLModel) ä¸æ¥å— pixel_valuesï¼Œåªæ¥å—å¤„ç†åçš„ inputs_embeds
        # - å‚è€ƒ: script/qwen/qwen_server.py:180-184
        is_vlm_frozen = not any(p.requires_grad for p in self.base_model.parameters())

        with torch.no_grad() if is_vlm_frozen else torch.enable_grad():
            outputs = self.base_model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )
            multi_layer_hidden_states = outputs.hidden_states[-4:]

            # Debug: check VLM hidden states
            for i, hs in enumerate(multi_layer_hidden_states):
                if torch.isnan(hs).any() or torch.isinf(hs).any():
                    print(f"    [VLM] WARNING: hidden_state layer {-4+i} contains nan/inf!")
                    print(f"      shape: {hs.shape}, nan: {torch.isnan(hs).sum()}, inf: {torch.isinf(hs).sum()}")

        # ç¼–ç å†å²å¸§ (å¦‚æœä½¿ç”¨)
        history_feat = None
        if self.use_history and self.history_encoder is not None and history_images is not None:
            # è½¬æ¢ List[List[PIL.Image]] -> [B, num_frames, 3, H, W] tensor
            if isinstance(history_images, list) and len(history_images) > 0 and isinstance(history_images[0], list):
                # VLMReplayBuffer format: List[List[PIL.Image]]
                import torchvision.transforms as T
                transform = T.Compose([
                    T.Resize((224, 224)),
                    T.ToTensor(),
                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])

                batch_history = []
                # ä» history_encoder è¯»å–å®é™…éœ€è¦çš„å¸§æ•°
                required_frames = self.num_history_frames  # ä» __init__ è¯»å–

                for batch_idx, hist_imgs in enumerate(history_images):
                    if hist_imgs is None or len(hist_imgs) == 0:
                        # ç”¨é›¶å¡«å……ï¼ˆä½¿ç”¨ float32ï¼Œä¸ history_encoder ä¸€è‡´ï¼‰
                        batch_history.append(torch.zeros(required_frames, 3, 224, 224, dtype=torch.float32))
                    else:
                        # æ™ºèƒ½å¤„ç†ï¼šæˆªå–æˆ–å¡«å……åˆ° required_frames
                        actual_frames = len(hist_imgs)

                        if actual_frames >= required_frames:
                            # æˆªå–æœ€æ–°çš„ required_frames å¸§
                            selected_imgs = hist_imgs[:required_frames]
                        else:
                            # å¡«å……ï¼šç”¨æœ€åä¸€å¸§é‡å¤å¡«å……
                            selected_imgs = hist_imgs + [hist_imgs[-1]] * (required_frames - actual_frames)

                        frames = [transform(img) for img in selected_imgs]
                        batch_history.append(torch.stack(frames))  # [required_frames, 3, H, W]

                # å…³é”®ï¼šè½¬æ¢ä¸º float32ï¼ˆä¸ history_encoder ä¸€è‡´ï¼Œä¸ç”¨ bfloat16ï¼‰
                history_images = torch.stack(batch_history).to(device=self.device, dtype=torch.float32)  # [B, required_frames, 3, H, W]

            is_history_frozen = not any(p.requires_grad for p in self.history_encoder.parameters())

            # ğŸ”§ DEBUG: æ£€æŸ¥ history_encoder è¾“å…¥å’Œå‚æ•°
            if torch.isnan(history_images).any():
                print(f"    [HistoryEncoder] WARNING: INPUT history_images contains NaN!")
                print(f"      shape: {history_images.shape}, nan count: {torch.isnan(history_images).sum()}")

            # æ£€æŸ¥ history_encoder å‚æ•°æ˜¯å¦æœ‰ NaN
            for name, param in self.history_encoder.named_parameters():
                if torch.isnan(param).any():
                    print(f"    [HistoryEncoder] WARNING: PARAM {name} contains NaN!")

            with torch.no_grad() if is_history_frozen else torch.enable_grad():
                history_feat = self.history_encoder(history_images)  # [B, history_dim]

            # ğŸ”§ DEBUG: æ£€æŸ¥è¾“å‡º
            if torch.isnan(history_feat).any():
                print(f"    [HistoryEncoder] WARNING: OUTPUT history_feat contains NaN!")
                print(f"      history_feat shape: {history_feat.shape}")
                print(f"      history_images dtype: {history_images.dtype}")
                print(f"      history_encoder dtype: {next(self.history_encoder.parameters()).dtype}")

        # DPTæå–ç‰¹å¾ (åŒ…å«å†å²ç‰¹å¾èåˆ)
        is_dpt_frozen = not any(p.requires_grad for p in self.dpt_head.parameters())

        with torch.no_grad() if is_dpt_frozen else torch.enable_grad():
            features = self._extract_dpt_features(multi_layer_hidden_states, history_feat)

        return features

    def _extract_dpt_features(self, multi_layer_hidden_states, history_feat=None):
        """
        æå–DPTçš„ä¸­é—´ç‰¹å¾ (256-d pooled)ï¼Œä¸ç»è¿‡æœ€åçš„MLPå›å½’å±‚
        """
        B, seq_len, _ = multi_layer_hidden_states[0].shape

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°† VLM è¾“å‡ºçš„ bfloat16 hidden states è½¬æ¢ä¸º float32
        # DPT head ç°åœ¨æ˜¯ float32ï¼Œéœ€è¦åŒ¹é…çš„è¾“å…¥ dtype
        multi_layer_hidden_states = [hs.float() for hs in multi_layer_hidden_states]

        # ğŸ”§ DEBUG: æ£€æŸ¥ DPT å‚æ•°æ˜¯å¦æœ‰ NaN
        dpt_has_nan = False
        for name, param in self.dpt_head.named_parameters():
            if torch.isnan(param).any():
                print(f"    [DPT] WARNING: PARAM {name} has NaN! requires_grad={param.requires_grad}")
                dpt_has_nan = True
        if dpt_has_nan:
            print(f"    [DPT] DPT head parameters have NaN!")

        # Step 1: æŠ•å½±æ‰€æœ‰å±‚åˆ°ç»Ÿä¸€ç‰¹å¾ç©ºé—´
        projected = []
        for i, (proj, hidden_state) in enumerate(zip(self.dpt_head.projections, multi_layer_hidden_states)):
            p = proj(hidden_state)
            if torch.isnan(p).any():
                print(f"    [DPT] WARNING: projection[{i}] output has NaN!")
            projected.append(p)

        # Step 2: è½¬æ¢ä¸ºConv1dæ ¼å¼ [B, feature_dim, seq_len]
        projected = [p.transpose(1, 2) for p in projected]

        # Step 3: æ¸è¿›å¼èåˆ (top-down refinement)
        fused = projected[-1]
        for i in range(len(self.dpt_head.fusion_blocks) - 1, -1, -1):
            skip = projected[i]
            fused = self.dpt_head.fusion_blocks[i](fused, skip)
            if torch.isnan(fused).any():
                print(f"    [DPT] WARNING: fusion_block[{i}] output has NaN!")

        fused = fused.transpose(1, 2)  # [B, seq_len, feature_dim]

        # Step 4: Spatial attention pooling
        attention_weights = self.dpt_head.spatial_attention(fused)  # [B, seq_len, 1]
        if torch.isnan(attention_weights).any():
            print(f"    [DPT] WARNING: attention_weights has NaN!")
        pooled = (fused * attention_weights).sum(dim=1)  # [B, feature_dim]

        # Step 5: å†å²ç‰¹å¾èåˆ (å¦‚æœä½¿ç”¨)
        if self.use_history and history_feat is not None:
            # ğŸ”§ DEBUG: æ£€æŸ¥ dtype å’Œ NaN
            if torch.isnan(pooled).any():
                print(f"    [DPT] WARNING: pooled contains NaN BEFORE history fusion!")
            if torch.isnan(history_feat).any():
                print(f"    [DPT] WARNING: history_feat contains NaN!")
                print(f"      pooled dtype: {pooled.dtype}, history_feat dtype: {history_feat.dtype}")

            # ç¡®ä¿ dtype ä¸€è‡´ï¼ˆéƒ½è½¬ä¸º float32 ä»¥ä¿è¯ç¨³å®šæ€§ï¼‰
            pooled = pooled.float()
            history_feat = history_feat.float()

            # ä¸ç›‘ç£å­¦ä¹ ä¿æŒä¸€è‡´çš„èåˆæ–¹å¼
            combined = torch.cat([pooled, history_feat], dim=-1)  # [B, feature_dim + history_dim]

            # history_fusion ä¹Ÿéœ€è¦ float32 è¾“å…¥
            pooled = self.dpt_head.history_fusion(combined.to(self.dpt_head.history_fusion[0].weight.dtype))  # [B, feature_dim]

            if torch.isnan(pooled).any():
                print(f"    [DPT] WARNING: pooled contains NaN AFTER history fusion!")
                # æ£€æŸ¥ history_fusion æƒé‡
                for i, layer in enumerate(self.dpt_head.history_fusion):
                    if hasattr(layer, 'weight') and torch.isnan(layer.weight).any():
                        print(f"      history_fusion[{i}] weight has NaN!")

        return pooled  # [B, 256]


class VLM_DPT_Actor(nn.Module):
    """
    VLM+DPT Actor - ç”¨äºFTRL

    æ¶æ„: VLM+DPTç‰¹å¾æå– â†’ FCå±‚ â†’ Action

    Args:
        feature_extractor: VLM_DPT_FeatureExtractorå®ä¾‹
        action_dim: åŠ¨ä½œç»´åº¦ (7ä¸ªå¯¼èˆªå‚æ•°)
    """
    def __init__(self, feature_extractor, action_dim=7, algorithm="DWA"):
        super().__init__()

        self.feature_extractor = feature_extractor
        self.action_dim = action_dim
        self.algorithm = algorithm.upper()  # å­˜å‚¨ç®—æ³•ç±»å‹

        # è¾“å‡ºå±‚: 256-d â†’ action_dim
        self.fc = nn.Linear(feature_extractor.feature_dim, action_dim)

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)

        # é‡è¦: Actorçš„FCå±‚ä¿æŒfloat32ï¼Œä¸è¦è½¬bfloat16
        # bfloat16ç²¾åº¦å¤ªä½ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®š
        # self._sync_dtype()  # ç¦ç”¨dtypeåŒæ­¥

    def _sync_dtype(self):
        """åŒæ­¥FCå±‚dtypeä¸feature_extractor"""
        if hasattr(self.feature_extractor, 'base_model'):
            target_dtype = self.feature_extractor.base_model.dtype
            self.fc = self.fc.to(dtype=target_dtype)

    def forward(self, images, prompt=None, history_images=None):
        """
        Args:
            images: PIL.Image list æˆ– [B, 3, H, W] tensor (å½“å‰å¸§)
                    æˆ– List[(PIL.Image, List[PIL.Image])] (VLMReplayBufferæ ¼å¼)
            prompt: str, å¯é€‰
            history_images: [B, num_frames, 3, H, W] tensor (å†å²å¸§ï¼Œå¯é€‰)

        Returns:
            action: [B, action_dim] å½’ä¸€åŒ–åˆ°[-1, 1]
        """
        # Handle VLMReplayBuffer format: List[(img, history_imgs, linear_vel, angular_vel)]
        if isinstance(images, list) and len(images) > 0 and isinstance(images[0], tuple):
            # Unpack tuple format
            imgs = [item[0] for item in images]
            hists = [item[1] for item in images]
            # Extract velocities for prompt generation
            linear_vels = [item[2] if len(item) > 2 else 0.0 for item in images]
            angular_vels = [item[3] if len(item) > 3 else 0.0 for item in images]

            features = self.feature_extractor(
                imgs, prompt, hists,
                linear_vels=linear_vels,
                angular_vels=angular_vels,
                algorithm=self.algorithm  # ä½¿ç”¨å®ä¾‹å±æ€§
            )
        else:
            # Normal format: separate images and history_images
            features = self.feature_extractor(images, prompt, history_images)

        # è½¬æ¢åˆ°float32ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ (FCå±‚æ˜¯float32)
        features = features.float()

        # Debug: check VLM features
        if torch.isnan(features).any() or torch.isinf(features).any():
            print(f"    [Actor] WARNING: VLM features contain nan/inf!")
            print(f"      features range: [{features.min():.4f}, {features.max():.4f}]")
            print(f"      nan count: {torch.isnan(features).sum()}, inf count: {torch.isinf(features).sum()}")

        # Debug: check FC weights
        if torch.isnan(self.fc.weight).any() or torch.isnan(self.fc.bias).any():
            print(f"    [Actor] WARNING: FC layer weights contain nan!")

        action = torch.tanh(self.fc(features))
        return action


class VLM_DPT_Critic(nn.Module):
    """
    VLM+DPT Critic - ç”¨äºFTRL

    æ¶æ„: VLM+DPTç‰¹å¾æå– + Actionç¼–ç  â†’ åŒQç½‘ç»œ

    TD3çš„Twin Criticè®¾è®¡: å…±äº«ç‰¹å¾æå–å™¨ï¼Œç‹¬ç«‹çš„Q-head
    (TwinæŒ‡çš„æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„Q-valueä¼°è®¡ï¼Œä¸æ˜¯ä¸¤ä¸ªç‰¹å¾æå–å™¨)

    Args:
        feature_extractor: å…±äº«çš„VLM_DPT_FeatureExtractor (ä¸Actorå…±äº«)
        action_dim: åŠ¨ä½œç»´åº¦
        detach_features: æ˜¯å¦detachç‰¹å¾ï¼ˆé˜²æ­¢Criticæ¢¯åº¦å›ä¼ åˆ°VLMï¼‰
    """
    def __init__(
        self,
        feature_extractor,
        action_dim=7,
        detach_features=True  # é»˜è®¤detachï¼Œä¿æŠ¤Actorçš„VLMæ›´æ–°
    ):
        super().__init__()

        # å…±äº«çš„ç‰¹å¾æå–å™¨ (ä¸Actorå…±äº«ï¼ŒVLMåªéœ€è¦è·‘1æ¬¡)
        self.feature_extractor = feature_extractor
        self.detach_features = detach_features

        feature_dim = feature_extractor.feature_dim

        # å…±äº«çš„Actionç¼–ç å™¨
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )

        # Q1ç½‘ç»œ: [256 + 64] â†’ 1
        self.q1_head = nn.Sequential(
            nn.Linear(feature_dim + 64, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # Q2ç½‘ç»œ: [256 + 64] â†’ 1
        self.q2_head = nn.Sequential(
            nn.Linear(feature_dim + 64, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # åˆå§‹åŒ–
        self._init_weights()

        # é‡è¦: Criticçš„å¯è®­ç»ƒå±‚ä¿æŒfloat32ï¼Œä¸è¦è½¬bfloat16
        # bfloat16ç²¾åº¦å¤ªä½ï¼ŒAdamæ›´æ–°æ—¶ä¼šå¯¼è‡´nan
        # VLMç‰¹å¾ä¼šè‡ªåŠ¨è½¬æ¢dtypeï¼Œä½†Q-headséœ€è¦float32ä¿è¯æ•°å€¼ç¨³å®šæ€§
        # self._sync_dtype()  # ç¦ç”¨dtypeåŒæ­¥

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in [self.action_encoder, self.q1_head, self.q2_head]:
            for m in module.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)

    def _sync_dtype(self):
        """åŒæ­¥æ‰€æœ‰å±‚çš„dtypeä¸feature_extractor"""
        if hasattr(self.feature_extractor, 'base_model'):
            target_dtype = self.feature_extractor.base_model.dtype
            self.action_encoder = self.action_encoder.to(dtype=target_dtype)
            self.q1_head = self.q1_head.to(dtype=target_dtype)
            self.q2_head = self.q2_head.to(dtype=target_dtype)

    def forward(self, images, action, prompt=None, history_images=None):
        """
        Args:
            images: PIL.Image list æˆ– [B, 3, H, W] tensor (å½“å‰å¸§)
                    æˆ– List[(PIL.Image, List[PIL.Image], float, float)] (VLMReplayBufferæ ¼å¼)
            action: [B, action_dim] åŠ¨ä½œå‚æ•°
            prompt: str, å¯é€‰
            history_images: [B, num_frames, 3, H, W] tensor (å†å²å¸§ï¼Œå¯é€‰)

        Returns:
            q1, q2: [B, 1] ä¸¤ä¸ªQå€¼ä¼°è®¡
        """
        # Handle VLMReplayBuffer format: List[(img, history_imgs, linear_vel, angular_vel)]
        if isinstance(images, list) and len(images) > 0 and isinstance(images[0], tuple):
            # Unpack tuple format
            imgs = [item[0] for item in images]
            hists = [item[1] for item in images]
            linear_vels = [item[2] if len(item) > 2 else 0.0 for item in images]
            angular_vels = [item[3] if len(item) > 3 else 0.0 for item in images]

            vlm_feat = self.feature_extractor(
                imgs, prompt, hists,
                linear_vels=linear_vels,
                angular_vels=angular_vels,
                algorithm=self.feature_extractor.algorithm  # ä»feature_extractorè·å–
            )
        else:
            # Normal format: separate images and history_images
            vlm_feat = self.feature_extractor(images, prompt, history_images)

        # å…³é”®: detaché˜»æ­¢Criticçš„æ¢¯åº¦å›ä¼ åˆ°VLMï¼Œä¿æŠ¤Actorçš„æ›´æ–°
        if self.detach_features:
            vlm_feat = vlm_feat.detach()

        # è½¬æ¢åˆ°float32ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ (Q-headsæ˜¯float32)
        vlm_feat = vlm_feat.float()
        action = action.float()

        action_feat = self.action_encoder(action)
        q_input = torch.cat([vlm_feat, action_feat], dim=-1)

        # ä¸¤ä¸ªç‹¬ç«‹çš„Q-head (TD3çš„Twin Critic)
        q1 = self.q1_head(q_input)
        q2 = self.q2_head(q_input)

        return q1, q2

    def Q1(self, images, action, prompt=None, history_images=None):
        """
        åªè®¡ç®—Q1 (ç”¨äºactoræ›´æ–°æ—¶çš„æ¢¯åº¦è®¡ç®—)

        Args:
            images: PIL.Image list æˆ– [B, 3, H, W] tensor (å½“å‰å¸§)
                    æˆ– List[(PIL.Image, List[PIL.Image], float, float)] (VLMReplayBufferæ ¼å¼)
            action: [B, action_dim]
            prompt: str, å¯é€‰
            history_images: [B, num_frames, 3, H, W] tensor (å†å²å¸§ï¼Œå¯é€‰)

        Returns:
            q1: [B, 1]
        """
        # Handle VLMReplayBuffer format: List[(img, history_imgs, linear_vel, angular_vel)]
        if isinstance(images, list) and len(images) > 0 and isinstance(images[0], tuple):
            # Unpack tuple format
            imgs = [item[0] for item in images]
            hists = [item[1] for item in images]
            linear_vels = [item[2] if len(item) > 2 else 0.0 for item in images]
            angular_vels = [item[3] if len(item) > 3 else 0.0 for item in images]

            # è·å–ç®—æ³•ç±»å‹ (ä»feature_extractoræˆ–é»˜è®¤DWA)
            algorithm = getattr(self.feature_extractor, 'algorithm', 'DWA')

            vlm_feat = self.feature_extractor(
                imgs, prompt, hists,
                linear_vels=linear_vels,
                angular_vels=angular_vels,
                algorithm=algorithm
            )
        else:
            # Normal format: separate images and history_images
            vlm_feat = self.feature_extractor(images, prompt, history_images)

        # å…³é”®: detaché˜»æ­¢Criticçš„æ¢¯åº¦å›ä¼ åˆ°VLMï¼Œä¿æŠ¤Actorçš„æ›´æ–°
        if self.detach_features:
            vlm_feat = vlm_feat.detach()

        # è½¬æ¢åˆ°float32ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ (Q-headsæ˜¯float32)
        vlm_feat = vlm_feat.float()
        action = action.float()

        action_feat = self.action_encoder(action)
        q1_input = torch.cat([vlm_feat, action_feat], dim=-1)
        return self.q1_head(q1_input)
