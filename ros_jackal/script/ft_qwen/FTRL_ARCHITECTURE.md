# FTRLæ¶æ„è¯¦è§£ - VLM+DPT Feature Extractorçš„256ç»´ç‰¹å¾

## ğŸ¯ æ ¸å¿ƒç†è§£

**`actor_feature_extractor` çš„è¾“å‡ºæ˜¯256ç»´ç‰¹å¾å‘é‡ï¼Œè¿™ä¸ª256ç»´æ¥è‡ªDPT Headçš„ä¸­é—´poolingæ“ä½œï¼ŒNOTæ˜¯æœ€ç»ˆçš„å‚æ•°é¢„æµ‹ï¼**

---

## ğŸ“ å®Œæ•´çš„æ•°æ®æµ

### ç›‘ç£å­¦ä¹ é˜¶æ®µï¼ˆqwen_dpt/lmms-finetune-qwenï¼‰

```python
# models/qwen2_5_vl_dpt_regression.py: DPTHead.forward()

[Costmapå›¾åƒ]
    â†“
Qwen2.5-VL-3B
    â†“ æå–æœ€å4å±‚hidden states
    â†“ [layer_-4, layer_-3, layer_-2, layer_-1]
    â†“
DPTHead.forward(multi_layer_hidden_states):

    # Step 1: æŠ•å½±åˆ°ç»Ÿä¸€256ç»´ç©ºé—´
    projected = [
        proj(hidden_state)  # [B, seq_len, 2048] â†’ [B, seq_len, 256]
        for proj, hidden_state in zip(self.projections, multi_layer_hidden_states)
    ]

    # Step 2: æ¸è¿›å¼èåˆï¼ˆtop-down refinementï¼‰
    projected = [p.transpose(1, 2) for p in projected]  # [B, 256, seq_len]
    fused = projected[-1]  # ä»æœ€é«˜å±‚å¼€å§‹
    for i in range(len(self.fusion_blocks) - 1, -1, -1):
        skip = projected[i]
        fused = self.fusion_blocks[i](fused, skip)  # é€å±‚èåˆ
    fused = fused.transpose(1, 2)  # [B, seq_len, 256]

    # Step 3: ç©ºé—´æ³¨æ„åŠ›æ± åŒ– âœ…âœ…âœ… å…³é”®ç‚¹ï¼
    attention_weights = self.spatial_attention(fused)  # [B, seq_len, 1]
    pooled = (fused * attention_weights).sum(dim=1)    # [B, 256] â† 256ç»´ç‰¹å¾ï¼

    # Step 4: å›å½’MLPï¼ˆç›‘ç£å­¦ä¹ ç”¨ï¼ŒRLä¸ç”¨ï¼ï¼‰
    predictions = self.mlp(pooled)  # [B, 256] â†’ [B, 7å‚æ•°]
    return predictions
```

**ç›‘ç£å­¦ä¹ è®­ç»ƒçš„checkpointåŒ…å«**ï¼š
```
checkpoint-5000/
â”œâ”€â”€ adapter_config.json              # LoRAé…ç½®
â”œâ”€â”€ adapter_model.safetensors        # LoRAæƒé‡ (330.91M)
â”œâ”€â”€ regression_head/
â”‚   â””â”€â”€ pytorch_model.bin            # DPT Head (3.89M)
â”‚       â”œâ”€â”€ projections.*            # Step 1çš„æŠ•å½±å±‚
â”‚       â”œâ”€â”€ fusion_blocks.*          # Step 2çš„èåˆå±‚
â”‚       â”œâ”€â”€ spatial_attention.*      # Step 3çš„æ³¨æ„åŠ›æ± åŒ–
â”‚       â””â”€â”€ mlp.*                    # Step 4çš„å›å½’MLP
â”œâ”€â”€ history_encoder/
â”‚   â””â”€â”€ pytorch_model.bin            # History Encoder (1.68M)
â”œâ”€â”€ history_config.json              # å†å²å¸§é…ç½®
â””â”€â”€ normalization/
    â”œâ”€â”€ param_mean.npy
    â””â”€â”€ param_std.npy
```

---

### RLå¾®è°ƒé˜¶æ®µï¼ˆrlft/vlm_net.pyï¼‰

```python
# vlm_net.py: VLM_DPT_FeatureExtractor

class VLM_DPT_FeatureExtractor:
    def __init__(self, checkpoint_path, freeze_vlm=True, freeze_dpt=False, ...):
        # 1. åŠ è½½VLM Base (2.03B, 4-bité‡åŒ–)
        self.base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-3B-Instruct",
            quantization_config=bnb_config,  # 4-bité‡åŒ–
            device_map=device
        )

        # 2. åŠ è½½LoRA (330.91M)
        if os.path.exists(adapter_config.json):
            self.base_model = PeftModel.from_pretrained(
                self.base_model,
                checkpoint_path,
                is_trainable=(not freeze_vlm)  # âœ… æ ¹æ®freeze_vlmå†³å®š
            )

            # å…³é”®å†³ç­–ï¼šæ˜¯å¦merge LoRA
            if freeze_vlm:
                # VLMå†»ç»“ï¼šmerge LoRAä»¥èŠ‚çœæ˜¾å­˜
                self.base_model = self.base_model.merge_and_unload()
                print("âœ“ LoRA merged into base model (VLM frozen)")
            else:
                # VLMå¯è®­ç»ƒï¼šä¿æŒLoRAä½œä¸ºç‹¬ç«‹å±‚ï¼Œä»¥ä¾¿è®­ç»ƒå’Œä¿å­˜
                print(f"âœ“ LoRA loaded as trainable layers: {lora_params:,} parameters")

        # 3. åŠ è½½DPT head (3.89M)
        self.dpt_head = DPTHead(
            hidden_size=2048,
            num_params=8,
            feature_dim=256,   # âœ… å…³é”®ï¼š256ç»´ç‰¹å¾
            num_layers=4,
            use_history=use_history
        )
        state_dict = torch.load(regression_head_path)
        self.dpt_head.load_state_dict(state_dict, strict=False)

        # 4. åŠ è½½History Encoder (1.68M, å¯é€‰)
        if use_history:
            self.history_encoder = HistoryEncoder(...)
            history_state_dict = torch.load(history_encoder_path)
            self.history_encoder.load_state_dict(history_state_dict)

        # 5. å†»ç»“ç­–ç•¥
        if freeze_vlm:
            for param in self.base_model.parameters():
                param.requires_grad = False
        if freeze_dpt:
            for param in self.dpt_head.parameters():
                param.requires_grad = False

    def forward(self, images, prompt=None, history_images=None):
        # VLMå‰å‘ä¼ æ’­
        outputs = self.base_model.model(**inputs, output_hidden_states=True)
        multi_layer_hidden_states = outputs.hidden_states[-4:]

        # å†å²å¸§å¤„ç† (å¯é€‰)
        history_feat = None
        if self.use_history and history_images is not None:
            history_feat = self.history_encoder(history_images)

        # âœ… å…³é”®ï¼šåªæå–åˆ°256ç»´ç‰¹å¾ï¼Œä¸ç»è¿‡mlpï¼
        features = self._extract_dpt_features(multi_layer_hidden_states, history_feat)
        return features  # [B, 256]

    def _extract_dpt_features(self, multi_layer_hidden_states, history_feat=None):
        """
        æå–DPTçš„ä¸­é—´ç‰¹å¾ (256-d pooled)ï¼Œä¸ç»è¿‡æœ€åçš„MLPå›å½’å±‚
        """
        # Step 1: æŠ•å½±æ‰€æœ‰å±‚åˆ°ç»Ÿä¸€ç‰¹å¾ç©ºé—´
        projected = [
            proj(hidden_state)
            for proj, hidden_state in zip(
                self.dpt_head.projections,
                multi_layer_hidden_states
            )
        ]

        # Step 2: è½¬æ¢ä¸ºConv1dæ ¼å¼ [B, feature_dim, seq_len]
        projected = [p.transpose(1, 2) for p in projected]

        # Step 3: æ¸è¿›å¼èåˆ (top-down refinement)
        fused = projected[-1]
        for i in range(len(self.dpt_head.fusion_blocks) - 1, -1, -1):
            skip = projected[i]
            fused = self.dpt_head.fusion_blocks[i](fused, skip)

        fused = fused.transpose(1, 2)  # [B, seq_len, feature_dim]

        # Step 4: Spatial attention pooling âœ…âœ…âœ… å°±æ˜¯è¿™é‡Œï¼
        attention_weights = self.dpt_head.spatial_attention(fused)  # [B, seq_len, 1]
        pooled = (fused * attention_weights).sum(dim=1)  # [B, 256] â† 256ç»´ç‰¹å¾ï¼

        # Step 5: å†å²ç‰¹å¾èåˆ (å¯é€‰)
        if self.use_history and history_feat is not None:
            combined = torch.cat([pooled, history_feat], dim=-1)  # [B, 512]
            pooled = self.dpt_head.history_fusion(combined)  # [B, 256]

        return pooled  # [B, 256]
        # âŒ æ³¨æ„ï¼šæ²¡æœ‰è°ƒç”¨ self.dpt_head.mlp ï¼


# vlm_net.py: VLM_DPT_Actor

class VLM_DPT_Actor:
    def __init__(self, feature_extractor, action_dim=7):
        self.feature_extractor = feature_extractor  # ä¸Šé¢çš„FeatureExtractor
        self.fc = nn.Linear(256, action_dim)  # âœ… æ–°çš„å†³ç­–å±‚ï¼256 â†’ 7

    def forward(self, images, prompt=None, history_images=None):
        features = self.feature_extractor(images, prompt, history_images)
        # features: [B, 256] â† æ¥è‡ªpooled
        action = torch.tanh(self.fc(features))  # [B, 7] â† ç”¨æ–°FCå±‚é¢„æµ‹
        return action
```

---

## ğŸ’¾ Checkpointä¿å­˜å’ŒåŠ è½½

### ä¸‰å±‚Checkpointç³»ç»Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ç›‘ç£å­¦ä¹ Checkpoint (åªè¯»)                            â”‚
â”‚    model/ddp/checkpoint-5000/                           â”‚
â”‚    â”œâ”€ VLM base (2.03B, ä»HuggingFace)                  â”‚
â”‚    â”œâ”€ LoRA (330.91M, adapter_model.safetensors)       â”‚
â”‚    â”œâ”€ DPT Head (3.89M, regression_head/)              â”‚
â”‚    â””â”€ History Encoder (1.68M, history_encoder/)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ å¼•ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RLè®­ç»ƒCheckpoint (è¯»å†™)                              â”‚
â”‚    logging/.../policy_step_1000_*                       â”‚
â”‚    â”œâ”€ policy_step_1000_actor (22MB)                    â”‚
â”‚    â”‚   â””â”€ DPT + History + FC è®­ç»ƒåå‚æ•°                â”‚
â”‚    â”œâ”€ policy_step_1000_vlm_info (154B) âœ…              â”‚
â”‚    â”‚   â””â”€ è®°å½•ç›‘ç£å­¦ä¹ checkpointè·¯å¾„                   â”‚
â”‚    â”œâ”€ policy_step_1000_noise (21B)                     â”‚
â”‚    â”‚   â””â”€ æ¢ç´¢å™ªå£°                                      â”‚
â”‚    â””â”€ policy_step_1000_lora_adapter/ (~1.3GB, å¯é€‰)   â”‚
â”‚        â””â”€ å¦‚æœfreeze_vlm=Falseï¼Œä¿å­˜LoRAæ›´æ–°          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ å®æ—¶åŒæ­¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Condorå®æ—¶Policy (ä¸´æ—¶, bufferç›®å½•)                  â”‚
â”‚    buffer/ddp_rlft/                                     â”‚
â”‚    â”œâ”€ policy_actor (22MB) - æœ€æ–°çš„DPT+FC               â”‚
â”‚    â”œâ”€ policy_vlm_info (154B) - VLMé…ç½®ä¿¡æ¯            â”‚
â”‚    â””â”€ policy_noise (21B) - æ¢ç´¢å™ªå£°                    â”‚
â”‚    ç”¨äºåœ¨çº¿æ•°æ®æ”¶é›†                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### policy_vlm_info çš„ä½œç”¨

**å®ƒæ˜¯ä¸€ä¸ªæŒ‡é’ˆæ–‡ä»¶**ï¼Œè®°å½•å¦‚ä½•é‡å»ºVLMï¼š

```python
# policy_vlm_info å†…å®¹
{
    'checkpoint_path': '/path/to/model/ddp/checkpoint-5000',  # â† VLMä»å“ªé‡ŒåŠ è½½
    'use_4bit': True,          # é‡æ–°åŠ è½½æ—¶ç”¨4-bité‡åŒ–
    'use_history': True,       # æ˜¯å¦ä½¿ç”¨History Encoder
    'vlm_trainable': False     # æ˜¯å¦æœ‰LoRAæ›´æ–°éœ€è¦åŠ è½½
}
```

**ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ**
- VLM base (2.03B) å› ä¸º4-bité‡åŒ–æ— æ³•ç”¨pickleä¿å­˜
- æ¯æ¬¡å¯åŠ¨éƒ½ä»HuggingFaceé‡æ–°åŠ è½½VLM base
- ç„¶åæ ¹æ®vlm_infoä»æ­£ç¡®çš„checkpointåŠ è½½LoRA
- ç¡®ä¿åœ¨ä¸åŒç¯å¢ƒ/è¿›ç¨‹ä¸­èƒ½æ­£ç¡®é‡å»ºå®Œæ•´æ¨¡å‹

### ä¿å­˜ç­–ç•¥ (rl.py:209-271)

```python
def save(self, dir, filename):
    """
    ä¿å­˜ç­–ç•¥ï¼š
    1. VLM base: è·³è¿‡ï¼ˆ4-bité‡åŒ–ï¼Œæ— æ³•pickleï¼‰
    2. LoRA adapters: å¦‚æœå¯è®­ç»ƒï¼Œä½¿ç”¨PEFTä¿å­˜ âœ… æ–°å¢
    3. DPT + History + FCï¼šä¿å­˜å¯è®­ç»ƒå‚æ•°
    """
    state_dict_to_save = {}

    for name, param in self.actor.named_parameters():
        # âŒ è·³è¿‡VLM baseå‚æ•°ï¼ˆ4-bité‡åŒ–ï¼Œæ— æ³•pickleï¼‰
        if 'feature_extractor.base_model' in name:
            continue

        # âœ… ä¿å­˜DPTã€Historyã€FCå‚æ•°
        state_dict_to_save[name] = param.detach().cpu()

    # ä¿å­˜DPT + History + FC
    pickle.dump(state_dict_to_save, f)  # â†’ policy_*_actor

    # ä¿å­˜æ¢ç´¢å™ªå£°
    pickle.dump(self.exploration_noise, f)  # â†’ policy_*_noise

    # âœ… æ£€æŸ¥VLMæ˜¯å¦æœ‰å¯è®­ç»ƒçš„LoRAå‚æ•°
    vlm_trainable = any(p.requires_grad for p in base_model.parameters())

    if vlm_trainable:
        # âœ… å¦‚æœVLMå¯è®­ç»ƒï¼Œä¿å­˜LoRA adapters
        from peft import PeftModel
        if isinstance(base_model, PeftModel):
            lora_save_path = join(dir, filename + "_lora_adapter")
            base_model.save_pretrained(lora_save_path)
            print(f"âœ“ LoRA adapters saved to {lora_save_path}")

    # ä¿å­˜VLMé…ç½®ä¿¡æ¯
    pickle.dump({
        'checkpoint_path': checkpoint_path,
        'use_4bit': True,
        'use_history': use_history,
        'vlm_trainable': vlm_trainable  # âœ… æ–°å¢
    }, f)  # â†’ policy_*_vlm_info
```

### åŠ è½½ç­–ç•¥ (rl.py:273-329)

```python
def load(self, dir, filename):
    """
    åŠ è½½ç­–ç•¥ï¼š
    1. VLM baseå·²åœ¨åˆå§‹åŒ–æ—¶ä»checkpointåŠ è½½
    2. åŠ è½½DPT + History + FCçš„è®­ç»ƒåå‚æ•°
    3. å¦‚æœæœ‰LoRAæ›´æ–°ï¼ŒåŠ è½½LoRA adapters âœ… æ–°å¢
    """
    # 1. åŠ è½½DPT + History + FC
    saved_state_dict = pickle.load(f)  # â† policy_*_actor
    self.actor.load_state_dict(saved_state_dict, strict=False)

    # 2. âœ… åŠ è½½LoRA adaptersï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    lora_save_path = join(dir, filename + "_lora_adapter")
    if exists(lora_save_path):
        from peft import PeftModel
        if isinstance(base_model, PeftModel):
            # å…ˆunloadæ—§LoRAï¼ŒåŠ è½½æ–°çš„
            base_model = base_model.unmerge_and_unload()
            base_model = PeftModel.from_pretrained(
                base_model,
                lora_save_path,
                is_trainable=True  # å¦‚æœç»§ç»­è®­ç»ƒ
            )
            print(f"âœ“ LoRA adapters loaded from {lora_save_path}")

    # 3. åŠ è½½å™ªå£°
    self.exploration_noise = pickle.load(f)  # â† policy_*_noise
```

### Collectorçš„åŸå­æ€§ä¿å­˜ (collector.py:641-656)

```python
def save_policy(self):
    """
    å°†å½“å‰policyä¿å­˜åˆ°buffer_path/
    ä½¿ç”¨åŸå­æ€§é‡å‘½åé¿å…race condition
    """
    # Step 1: ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
    self.policy.save(self.buffer_path, "policy_copy")
    # åˆ›å»º: policy_copy_actor, policy_copy_noise, policy_copy_vlm_info

    # Step 2: åŸå­æ€§é‡å‘½åï¼ˆé˜²æ­¢actorè¯»å–åˆ°æŸåçš„æ–‡ä»¶ï¼‰
    shutil.move(
        join(self.buffer_path, "policy_copy_actor"),
        join(self.buffer_path, "policy_actor")  # â† æœ€ç»ˆæ–‡ä»¶
    )
    shutil.move(
        join(self.buffer_path, "policy_copy_noise"),
        join(self.buffer_path, "policy_noise")
    )
    shutil.move(
        join(self.buffer_path, "policy_copy_vlm_info"),
        join(self.buffer_path, "policy_vlm_info")  # âœ… ä¿®å¤ï¼šä¹‹å‰å¿˜è®°é‡å‘½å
    )
```

---

## ğŸ”§ ä¸¤ç§è®­ç»ƒæ¨¡å¼

### æ¨¡å¼1: VLMå†»ç»“ (freeze_vlm=True) - å½“å‰é»˜è®¤ âœ…

```python
VLM Base (2.03B)  â†’ å†»ç»“ â†’ merge LoRA â†’ æ— éœ€ä¿å­˜
LoRA (330.91M)    â†’ åˆå¹¶åˆ°base â†’ æ— éœ€ä¿å­˜
DPT (3.89M)       â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
History (1.68M)   â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
FC (2K)           â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä¿å­˜å¤§å°: 22MB (åªæœ‰DPT + History + FC)
```

**ä¿å­˜æ–‡ä»¶**:
```
logging/.../
â”œâ”€â”€ policy_step_1000_actor        # 22MB: DPT + History + FC
â”œâ”€â”€ policy_step_1000_noise        # æ¢ç´¢å™ªå£°
â””â”€â”€ policy_step_1000_vlm_info     # VLMè·¯å¾„ä¿¡æ¯
```

**å¯åŠ¨å‚æ•°**:
```yaml
training_config:
  freeze_vlm_actor: true   # VLMå†»ç»“
  freeze_dpt_actor: false  # DPTå¯è®­ç»ƒ
  actor_lr: 1.0e-5
```

### æ¨¡å¼2: VLMå¯è®­ç»ƒ (freeze_vlm=False) - æ¶ˆèå®éªŒ

```python
VLM Base (2.03B)  â†’ å†»ç»“ â†’ æ— éœ€ä¿å­˜ (ä»HFåŠ è½½)
LoRA (330.91M)    â†’ å¯è®­ç»ƒ â†’ ä¿å­˜LoRA adapters âœ…
DPT (3.89M)       â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
History (1.68M)   â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
FC (2K)           â†’ å¯è®­ç»ƒ â†’ ä¿å­˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä¿å­˜å¤§å°: ~1.3GB (LoRA 1.3GB + DPTç­‰ 22MB)
```

**ä¿å­˜æ–‡ä»¶**:
```
logging/.../
â”œâ”€â”€ policy_step_1000_actor           # 22MB: DPT + History + FC
â”œâ”€â”€ policy_step_1000_lora_adapter/   # ~1.3GB: LoRAå¢é‡å‚æ•° âœ…
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors
â”œâ”€â”€ policy_step_1000_noise           # æ¢ç´¢å™ªå£°
â””â”€â”€ policy_step_1000_vlm_info        # VLMè·¯å¾„ä¿¡æ¯ (vlm_trainable=True)
```

**å¯åŠ¨å‚æ•°**:
```yaml
training_config:
  freeze_vlm_actor: false  # VLMå¯è®­ç»ƒ âœ…
  freeze_dpt_actor: false  # DPTå¯è®­ç»ƒ
  actor_lr: 5.0e-6         # æ›´å°çš„å­¦ä¹ ç‡ä¿æŠ¤é¢„è®­ç»ƒ
```

---

## ğŸ“Š å‚æ•°ç»Ÿè®¡

### å®Œæ•´æ¨¡å‹å‚æ•°åˆ†å¸ƒ (freeze_vlm=True)

```
======================================================================
ğŸ“Š VLM_DPT_FeatureExtractor Parameter Summary
======================================================================
VLM Base (Qwen2.5-VL-3B):
  â”œâ”€ Total:      2,034,024,448 parameters (2.03B)
  â”œâ”€ Trainable:             0 parameters (0.00M)
  â””â”€ Frozen:     2,034,024,448 parameters (2.03B)

DPT Head:
  â”œâ”€ Total:         3,891,337 parameters (3.89M)
  â”œâ”€ Trainable:     3,891,337 parameters (3.89M)
  â””â”€ Frozen:                0 parameters (0.00M)

History Encoder:
  â”œâ”€ Total:         1,676,352 parameters (1.68M)
  â”œâ”€ Trainable:     1,676,352 parameters (1.68M)
  â””â”€ Frozen:                0 parameters (0.00M)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Feature Extractor:
  â”œâ”€ Total:      2,039,592,137 parameters (2.04B)
  â”œâ”€ Trainable:     5,567,689 parameters (5.57M)
  â”œâ”€ Frozen:     2,034,024,448 parameters (2.03B)
  â””â”€ Trainable%:        0.27%
======================================================================
```

### æ–‡ä»¶å¤§å°éªŒè¯

```python
# ç†è®ºè®¡ç®—
5.57M å‚æ•° Ã— 4 bytes (float32) = 22.28MB

# å®é™…æ–‡ä»¶
buffer/ddp_rlft/policy_actor: 22MB âœ… å®Œå…¨åŒ¹é…ï¼
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **256ç»´ç‰¹å¾ = DPT Headçš„ç©ºé—´æ³¨æ„åŠ›æ± åŒ–ç»“æœ**
   ```python
   pooled = (fused * attention_weights).sum(dim=1)  # [B, 256]
   ```

2. **ç›‘ç£å­¦ä¹ å’ŒRLçš„åˆ†å·¥**
   - ç›‘ç£å­¦ä¹ ï¼šè®­ç»ƒåœºæ™¯ç†è§£ï¼ˆVLM + DPTå‰åŠéƒ¨åˆ†ï¼‰
   - RLå¾®è°ƒï¼šè®­ç»ƒå†³ç­–ç­–ç•¥ï¼ˆActor.fcå±‚ + å¯é€‰DPTå¾®è°ƒï¼‰

3. **ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**
   - åœºæ™¯ç†è§£æ˜¯é€šç”¨çš„ï¼ˆå¯ä»¥ä»ç›‘ç£å­¦ä¹ å¤ç”¨ï¼‰
   - å†³ç­–ç­–ç•¥æ˜¯ä»»åŠ¡ç›¸å…³çš„ï¼ˆRLé‡æ–°å­¦ä¹ ï¼‰
   - æ ·æœ¬æ•ˆç‡é«˜ï¼ˆå› ä¸ºåœºæ™¯ç†è§£å·²ç»å¾ˆå¼ºäº†ï¼‰

### ä¿å­˜ç­–ç•¥æ±‡æ€»

| ç»„ä»¶ | å‚æ•°é‡ | æ˜¯å¦ä¿å­˜ | å¤§å° | åŸå›  |
|------|-------|---------|------|------|
| **VLM Base** | 2.03B | âŒ ä¸ä¿å­˜ | 0 MB | 4-bité‡åŒ–ï¼Œä»HFé‡æ–°åŠ è½½ |
| **LoRA (å†»ç»“)** | 330.91M | âŒ ä¸ä¿å­˜ | 0 MB | åˆå¹¶åˆ°VLMï¼Œä»ç›‘ç£å­¦ä¹ åŠ è½½ |
| **LoRA (å¯è®­ç»ƒ)** | 330.91M | âœ… ä¿å­˜ | ~1.3 GB | ä½¿ç”¨PEFT.save_pretrained() |
| **DPT Head** | 3.89M | âœ… ä¿å­˜ | 15.56 MB | è®­ç»ƒæ›´æ–° |
| **History Encoder** | 1.68M | âœ… ä¿å­˜ | 6.72 MB | è®­ç»ƒæ›´æ–° |
| **FCå±‚** | 2K | âœ… ä¿å­˜ | 0.008 MB | è®­ç»ƒæ›´æ–° |

### æŠ€æœ¯è¦ç‚¹

1. **4-bité‡åŒ–çš„VLMæ— æ³•ç”¨pickleä¿å­˜**
   - åªèƒ½ä»HuggingFaceé‡æ–°åŠ è½½
   - ä½¿ç”¨policy_vlm_infoè®°å½•checkpointè·¯å¾„

2. **LoRAçš„æ¡ä»¶ä¿å­˜**
   - freeze_vlm=True: mergeåä¸ä¿å­˜
   - freeze_vlm=False: ä½¿ç”¨PEFTä¿å­˜adapter

3. **åŸå­æ€§ä¿å­˜**
   - ä½¿ç”¨policy_copy_*ä¸´æ—¶æ–‡ä»¶
   - åŸå­æ€§é‡å‘½åé˜²æ­¢race condition

---

**æœ€åæ›´æ–°**: 2026-01-19
**å…³é”®æ–‡ä»¶**:
- `src/ros_jackal/rlft/vlm_net.py`: VLM_DPT_FeatureExtractorå’ŒActorå®šä¹‰
- `src/ros_jackal/rlft/rl.py`: TD3 save/loadé€»è¾‘
- `src/ros_jackal/rlft/collector.py`: Condorä¿å­˜é€»è¾‘
