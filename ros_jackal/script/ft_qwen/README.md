# FTRL - VLM+DPTå¼ºåŒ–å­¦ä¹ å¾®è°ƒ

åŸºäºé¢„è®­ç»ƒVLM+DPTè¿›è¡ŒTD3å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„å®Œæ•´å®ç°

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
ros_jackal/
â”œâ”€â”€ rlft/                           # FTRLæ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ vlm_net.py                  # VLM+DPTç½‘ç»œå®šä¹‰
â”‚   â”œâ”€â”€ rl.py                       # TD3ç®—æ³•å®ç°
â”‚   â”œâ”€â”€ train.py                    # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ README.md                   # æŠ€æœ¯æ–‡æ¡£
â”‚
â”œâ”€â”€ script/ft_qwen/                 # é…ç½®å’Œå¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ ftrl_vlm_dwa.yaml      # DWAé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ run_ftrl.sh                # å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
â”‚
â”œâ”€â”€ buffer/ftrl_vlm/               # Replay bufferå­˜å‚¨
â””â”€â”€ logging/ftrl_vlm/              # TensorBoardæ—¥å¿—
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

**ç›‘ç£å­¦ä¹ checkpointå·²å®Œæˆ**ï¼š
- è·¯å¾„ç¤ºä¾‹: `/path/to/lmms-finetune-qwen/output/checkpoint-2500`
- åŒ…å«: LoRA adapters + DPT Head + (å¯é€‰) History Encoder

### é˜¶æ®µ1: æ•°æ®æ”¶é›†

**Step 1: å¯åŠ¨ Qwen æ¨ç†æœåŠ¡**

```bash
# ç»ˆç«¯1: å¯åŠ¨ qwen_server.py
cd /path/to/ros_jackal
python script/qwen/qwen_server.py \
  --base_model /path/to/Qwen2.5-VL-7B \
  --lora_path /path/to/checkpoint-2500 \
  --algorithm DWA \
  --port 5000
```

**Step 2: å¯åŠ¨æ•°æ®æ”¶é›†è„šæœ¬**

```bash
# ç»ˆç«¯2: å¯åŠ¨æ•°æ®æ”¶é›†
cd /path/to/ros_jackal
python script/ft_qwen/evaluate_ftrl_single.py \
  --id 0 \
  --server_url http://localhost:5000 \
  --policy_name dwa_ftrl \
  --buffer_path ./buffer/
```

**æ•°æ®ä¿å­˜ä½ç½®**: `buffer/dwa_ftrl/actor_0/`
- `traj_*.pickle`: è½¨è¿¹æ•°æ® (obs, action, reward, done)
- `*.png`: Costmap å›¾åƒ
- `trajectory_results.txt`: æ€§èƒ½ç»Ÿè®¡

### é˜¶æ®µ2: RL è®­ç»ƒ

**ä¿®æ”¹é…ç½®æ–‡ä»¶** `configs/ftrl_vlm_dwa.yaml`:

```yaml
training_config:
  vlm_checkpoint_path: "/path/to/checkpoint-2500"  # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
```

**å¯åŠ¨è®­ç»ƒ**:

```bash
cd /path/to/ros_jackal
./script/ft_qwen/run_ftrl.sh
```

### é˜¶æ®µ3: ç›‘æ§è¿›åº¦

```bash
tensorboard --logdir logging/ftrl_vlm
# æµè§ˆå™¨è®¿é—®: http://localhost:6006
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### å…³é”®æŒ‡æ ‡ï¼ˆTensorBoardï¼‰

- `train/Test_nav_metric`: æµ‹è¯•å¯¼èˆªæ€§èƒ½ï¼ˆâ†‘è¶Šé«˜è¶Šå¥½ï¼‰
- `train/Success_rate`: æˆåŠŸç‡ï¼ˆ%ï¼‰
- `train/Test_length`: å¹³å‡è½¨è¿¹é•¿åº¦ï¼ˆâ†“è¶ŠçŸ­è¶Šå¥½ï¼‰
- `train/Actor_loss`: ActoræŸå¤±
- `train/Critic_loss`: CriticæŸå¤±

### ç»ˆç«¯è¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¼šåœ¨ç»ˆç«¯è¾“å‡ºå®æ—¶ç»Ÿè®¡ï¼š

```
Episode_reward: 0.85
Episode_nav_metric: 0.92
Success_rate: 78.5%
Actor_loss: -0.34
Critic_loss: 0.012
fps: 245.3
```

## âš™ï¸ é…ç½®å‚æ•°è¯¦è§£

### å¿…é¡»ä¿®æ”¹çš„å‚æ•°

```yaml
training_config:
  # VLM+DPT checkpointè·¯å¾„ï¼ˆå¿…é¡»ä¿®æ”¹ï¼ï¼‰
  vlm_checkpoint_path: "/path/to/checkpoint"
```

### æ¨èçš„è¶…å‚æ•°

```yaml
training_config:
  # å†»ç»“ç­–ç•¥ï¼ˆæ¨èé…ç½®ï¼‰
  freeze_vlm_actor: true     # VLMå¤ªå¤§ï¼Œå¿…é¡»å†»ç»“
  freeze_dpt_actor: false    # DPTå¾®è°ƒï¼ˆFTRLæ ¸å¿ƒï¼‰
  freeze_dpt_critic: true    # Critic DPTå†»ç»“çœæ˜¾å­˜

  # å­¦ä¹ ç‡ï¼ˆVLMå¾®è°ƒéœ€è¦å°lrï¼‰
  actor_lr: 1.0e-5           # ä¸è¦è¶…è¿‡1e-4
  critic_lr: 3.0e-4

  # è®­ç»ƒæ­¥æ•°
  training_args:
    max_step: 1000000        # æ€»æ­¥æ•°
    collect_per_step: 1000   # æ¯æ¬¡æ”¶é›†
    update_per_step: 50      # æ¯æ¬¡æ›´æ–°
```

### æ€§èƒ½è°ƒä¼˜å‚æ•°

**æ˜¾å­˜ä¼˜åŒ–**:
```yaml
training_args:
  batch_size: 128           # é»˜è®¤256ï¼Œæ˜¾å­˜ä¸è¶³æ”¹ä¸º128
```

**è®­ç»ƒç¨³å®šæ€§**:
```yaml
training_config:
  exploration_noise_start: 0.02  # å‡å°æ¢ç´¢å™ªå£°
  actor_lr: 5.0e-6               # å‡å°å­¦ä¹ ç‡
  pre_collect: 50000             # å¢åŠ é¢„æ”¶é›†
```

## ğŸ¯ å®éªŒæŒ‡å—

### åŸºç¡€å®éªŒï¼ˆéªŒè¯å¯è¡Œæ€§ï¼‰

1. **å…ˆè·‘1å°æ—¶æµ‹è¯•**
   ```yaml
   training_args:
     max_step: 50000  # çº¦1å°æ—¶
   ```

2. **æ£€æŸ¥æŒ‡æ ‡**
   - `Test_nav_metric` æ˜¯å¦æå‡ï¼Ÿ
   - `Success_rate` æ˜¯å¦>ç›‘ç£å­¦ä¹ ï¼Ÿ

3. **å¦‚æœæŒ‡æ ‡ä¸‹é™**
   - å‡å° `actor_lr: 5e-6`
   - å‡å° `exploration_noise_start: 0.02`

### å®Œæ•´å®éªŒï¼ˆå‘è®ºæ–‡ï¼‰

1. **é•¿æ—¶é—´è®­ç»ƒ**
   ```yaml
   training_args:
     max_step: 1000000  # ~24å°æ—¶
   ```

2. **ä¿å­˜checkpoint**
   - æœ€ä½³æ¨¡å‹è‡ªåŠ¨ä¿å­˜åœ¨ `logging/ftrl_vlm/.../policy_step_XXX_actor`

3. **è¯„ä¼°å¯¹æ¯”**
   ```bash
   # è¯„ä¼°FTRLæ¨¡å‹
   python eval_ftrl.py --checkpoint logging/.../policy_step_XXX_actor

   # å¯¹æ¯”ç›‘ç£å­¦ä¹ 
   python eval_supervised.py --checkpoint /path/to/supervised/checkpoint
   ```

### æ¶ˆèå®éªŒï¼ˆåˆ†æè´¡çŒ®ï¼‰

**å®éªŒ1: VLMå†»ç»“ vs å¾®è°ƒ**
```yaml
# Exp A: VLMå†»ç»“ï¼ˆbaselineï¼‰
freeze_vlm_actor: true

# Exp B: VLMå¾®è°ƒ
freeze_vlm_actor: false
actor_lr: 5.0e-6  # æ›´å°çš„lr
```

**å®éªŒ2: DPTå†»ç»“ vs å¾®è°ƒ**
```yaml
# Exp A: DPTå†»ç»“ï¼ˆä¸åšFTRLï¼‰
freeze_dpt_actor: true

# Exp B: DPTå¾®è°ƒï¼ˆFTRLï¼‰
freeze_dpt_actor: false
```

**å®éªŒ3: ä¸åŒå­¦ä¹ ç‡**
```yaml
actor_lr: [1e-6, 5e-6, 1e-5, 5e-5]
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒå¯åŠ¨å¤±è´¥

**é”™è¯¯**: `ModuleNotFoundError: No module named 'qwen2_5_vl_dpt_regression'`

**è§£å†³**:
```bash
# æ£€æŸ¥è·¯å¾„
export PYTHONPATH="/path/to/qwen_dpt/lmms-finetune-qwen/models:$PYTHONPATH"
```

---

**é”™è¯¯**: `FileNotFoundError: checkpoint-2500 not found`

**è§£å†³**:
```yaml
# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
vlm_checkpoint_path: "/absolute/path/to/checkpoint-2500"
```

### Q2: æ˜¾å­˜æº¢å‡º

**é”™è¯¯**: `CUDA out of memory`

**è§£å†³**:
```yaml
# æ–¹æ¡ˆ1: å‡å°batch size
training_args:
  batch_size: 64  # é»˜è®¤256

# æ–¹æ¡ˆ2: å†»ç»“æ›´å¤šå‚æ•°
freeze_dpt_actor: true
freeze_dpt_critic: true

# æ–¹æ¡ˆ3: ä½¿ç”¨4-bité‡åŒ–ï¼ˆä¿®æ”¹vlm_net.pyï¼‰
```

### Q3: æ€§èƒ½ä¸æå‡

**ç°è±¡**: `Test_nav_metric` ä¸å¢é•¿æˆ–ä¸‹é™

**è¯Šæ–­**:
1. æ£€æŸ¥ `Actor_loss`: å¦‚æœæ˜¯NaN â†’ å­¦ä¹ ç‡å¤ªå¤§
2. æ£€æŸ¥ `Success_rate`: å¦‚æœ<50% â†’ æ¢ç´¢ä¸å¤Ÿ
3. æ£€æŸ¥ `Exploration_noise`: å¦‚æœå¤ªå° â†’ æ— æ³•æ¢ç´¢

**è§£å†³**:
```yaml
# å¦‚æœlossæ˜¯NaN
actor_lr: 5.0e-6  # å‡å°10å€

# å¦‚æœæˆåŠŸç‡å¤ªä½
exploration_noise_start: 0.1  # å¢åŠ æ¢ç´¢
pre_collect: 50000            # å¤šæ”¶é›†ç»éªŒ

# å¦‚æœå®Œå…¨ä¸å­¦ä¹ 
freeze_dpt_actor: false  # ç¡®ä¿DPTå¯è®­ç»ƒ
```

### Q4: è®­ç»ƒå¤ªæ…¢

**ç°è±¡**: fps < 50

**åŸå› **: VLMæ¨ç†æ…¢

**è§£å†³**:
```yaml
# æ–¹æ¡ˆ1: å‡å°æ›´æ–°é¢‘ç‡
training_args:
  update_per_step: 20  # é»˜è®¤50

# æ–¹æ¡ˆ2: å¢åŠ æ”¶é›†é¢‘ç‡
training_args:
  collect_per_step: 2000  # é»˜è®¤1000
```

## ğŸ“ˆ é¢„æœŸç»“æœ

### ç›‘ç£å­¦ä¹ baseline
- MAE: 0.05-0.1
- æˆåŠŸç‡: 70-80%
- æ¨ç†é€Ÿåº¦: 100-500ms

### FTRLç›®æ ‡
- æˆåŠŸç‡: >80%ï¼ˆè¶…è¿‡ç›‘ç£å­¦ä¹ ï¼‰
- æ ·æœ¬æ•ˆç‡: <1M steps
- è®­ç»ƒæ—¶é—´: ~24å°æ—¶

### APPLRå¯¹æ¯”
- APPLRæ ·æœ¬: 5M steps
- APPLRæ—¶é—´: 6å°æ—¶ï¼ˆ500 CPUå¹¶è¡Œï¼‰
- FTRLä¼˜åŠ¿: é¢„è®­ç»ƒåŠ æŒï¼Œæ ·æœ¬æ•ˆç‡é«˜

## ğŸ“ å‘è®ºæ–‡Checklist

- [ ] å¯¹æ¯”ç›‘ç£å­¦ä¹ æ€§èƒ½
- [ ] å¯¹æ¯”APPLRæ ·æœ¬æ•ˆç‡
- [ ] æ¶ˆèå®éªŒï¼ˆVLMå†»ç»“/å¾®è°ƒï¼ŒDPTå†»ç»“/å¾®è°ƒï¼‰
- [ ] æ³›åŒ–å®éªŒï¼ˆæµ‹è¯•ç¯å¢ƒ vs è®­ç»ƒç¯å¢ƒï¼‰
- [ ] å¯è§†åŒ–ï¼ˆè½¨è¿¹ã€attention mapï¼‰
- [ ] æ€§èƒ½åˆ†æï¼ˆæ¨ç†é€Ÿåº¦ã€æ˜¾å­˜å ç”¨ï¼‰

## ğŸ§  æ ¸å¿ƒåŸç†ï¼šRLå¦‚ä½•æ›´æ–°VLM+DPT

### é—®é¢˜ï¼šæ²¡æœ‰ground truthï¼Œå¦‚ä½•åå‘ä¼ æ’­ï¼Ÿ

**ç›‘ç£å­¦ä¹ ** (å·²å®Œæˆ):
```python
predicted_params = VLM_DPT(image)  # [7ä¸ªå‚æ•°]
loss = MSE(predicted_params, ground_truth)  # æœ‰æ˜ç¡®ç›®æ ‡
loss.backward()  # æ¢¯åº¦æ¸…æ™°
```

**å¼ºåŒ–å­¦ä¹ ** (FTRL):
```python
predicted_params = VLM_DPT(image)  # [7ä¸ªå‚æ•°]
reward = env.step(predicted_params)  # åªçŸ¥é“å¥½åï¼Œæ— ground truth
# é—®é¢˜ï¼šå¦‚ä½•è®¡ç®—æ¢¯åº¦ï¼Ÿ
```

### ç­”æ¡ˆï¼šé€šè¿‡Criticä½œä¸º"learned ground truth"

#### å®Œæ•´æ¢¯åº¦æµ

```python
# ========== å‰å‘ä¼ æ’­ ==========
# 1. Actor (VLM+DPT) ç”Ÿæˆaction
image â†’ VLM â†’ DPT â†’ FC â†’ action [7ä¸ªå‚æ•°]

# 2. Criticè¯„ä¼°actionçš„ä»·å€¼
Q = Critic(image, action)  # Qå€¼ = "è¿™ä¸ªactionçš„é•¿æœŸä»·å€¼"

# ========== åå‘ä¼ æ’­ ==========
# 3. Actor Loss
actor_loss = -Q  # è´Ÿå·ï¼šæƒ³æœ€å¤§åŒ–Qå€¼

# 4. æ¢¯åº¦åå‘ä¼ æ’­åˆ°VLM+DPT
actor_loss.backward()
    â†“ PyTorchè‡ªåŠ¨å¾®åˆ†
âˆ‚(-Q)/âˆ‚action â†’ actionçš„æ¢¯åº¦ (Criticå‘Šè¯‰æˆ‘ä»¬"å¦‚ä½•è°ƒæ•´action")
    â†“ é€šè¿‡FCå±‚
âˆ‚action/âˆ‚features â†’ DPT featuresçš„æ¢¯åº¦
    â†“ é€šè¿‡DPT Head
âˆ‚features/âˆ‚hidden_states â†’ VLMçš„æ¢¯åº¦
    â†“
VLM+DPTå‚æ•°æ›´æ–°ï¼
```

#### æ•°å­¦è¡¨è¾¾

```
æ¢¯åº¦ = âˆ‚(-Q)/âˆ‚Î¸_VLM+DPT
     = -âˆ‚Q(s,a)/âˆ‚a Ã— âˆ‚a/âˆ‚Î¸

å…¶ä¸­:
- âˆ‚Q/âˆ‚a: Criticè¯´"è°ƒæ•´actionçš„æ–¹å‘"
- âˆ‚a/âˆ‚Î¸: Actorè¯´"è°ƒæ•´å‚æ•°èƒ½äº§ç”Ÿé‚£ä¸ªaction"
- Î¸: VLM+DPTçš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
```

### Criticå¦‚ä½•å­¦ä¹ Qå€¼ï¼Ÿ

```python
# Criticé€šè¿‡Bellmanæ–¹ç¨‹ä»rewardå­¦ä¹ 
target_Q = reward + Î³ Ã— Q_target(next_state, next_action)
critic_loss = MSE(Q(state, action), target_Q)
critic_loss.backward()  # æ›´æ–°Critic

# Criticçš„ä½œç”¨:
# 1. å¯†é›†åŒ–reward (ç¯å¢ƒåªåœ¨ç»“æŸç»™rewardï¼ŒCriticèƒ½ä¼°è®¡æ¯æ­¥ä»·å€¼)
# 2. å¹³æ»‘æ¢¯åº¦ (ç›´æ¥ç”¨rewardå¾ˆä¸ç¨³å®š)
# 3. é•¿æœŸè§„åˆ’ (è€ƒè™‘æœªæ¥rewardï¼Œé€šè¿‡Î³æŠ˜æ‰£)
```

### å¯¹æ¯”ï¼šç›‘ç£å­¦ä¹  vs RL

| æ–¹é¢ | ç›‘ç£å­¦ä¹  | å¼ºåŒ–å­¦ä¹  (FTRL) |
|------|---------|----------------|
| **ç›®æ ‡** | æ¨¡ä»¿ä¸“å®¶æ•°æ® | æœ€å¤§åŒ–ç¯å¢ƒreward |
| **Lossæ¥æº** | `MSE(pred, ground_truth)` | `-Q(s, Actor(s))` |
| **æ¢¯åº¦ä¿¡å·** | æ˜ç¡®ä¸”ç¨³å®š | é€šè¿‡Criticé—´æ¥è·å¾— |
| **ä¼˜åŒ–ç›®æ ‡** | `min MSE` | `max E[Q(s,Ï€(s))]` |
| **æ•°æ®éœ€æ±‚** | å¤§é‡æ ‡æ³¨æ•°æ® | ç¯å¢ƒäº¤äº’ |
| **æ½œåŠ›** | å—é™äºæ ‡æ³¨è´¨é‡ | å¯èƒ½è¶…è¶Šäººç±» |

### ä¸ºä»€ä¹ˆCriticä¸ä¿å­˜ï¼Ÿ

```python
# APPLRå’ŒFTRLéƒ½åªä¿å­˜Actor
def save(self, dir, filename):
    with open(join(dir, filename + "_actor"), "wb") as f:
        pickle.dump(self.actor.state_dict(), f)
    # ä¸ä¿å­˜Criticï¼

# åŸå› :
# 1. Criticåªåœ¨è®­ç»ƒæ—¶ç”¨ (è®¡ç®—Qå€¼æŒ‡å¯¼Actor)
# 2. æ¨ç†/æ•°æ®æ”¶é›†æ—¶ä¸éœ€è¦Critic
# 3. ä¸‹æ¬¡è®­ç»ƒå¯ä»¥é‡æ–°åˆ›å»ºCritic (æˆ–åŠ è½½ä¸Šæ¬¡çš„ç»§ç»­è®­ç»ƒ)
```

### å®Œæ•´çš„FTRLæµç¨‹

```
é˜¶æ®µ1: ç›‘ç£å­¦ä¹  (å·²å®Œæˆ)
â”œâ”€ æ•°æ®: (costmap, optimal_params) pairs
â”œâ”€ è®­ç»ƒ: MSE(VLM_DPT(image), optimal_params)
â””â”€ ç»“æœ: åŸºç¡€çš„å‚æ•°é¢„æµ‹èƒ½åŠ›

é˜¶æ®µ2: æ•°æ®æ”¶é›† (Python 3.8 + ROS)
â”œâ”€ qwen_server.py (Python 3.10): æä¾›VLM+DPTæ¨ç†æœåŠ¡
â”‚   â””â”€ åŠ è½½ç›‘ç£å­¦ä¹ çš„checkpoint (é˜¶æ®µ1çš„è¾“å‡º)
â”œâ”€ evaluate_ftrl_single.py:
â”‚   â”œâ”€ result = qwen_client.infer_from_server(image_path)
â”‚   â”œâ”€ action = qwen_client.get_parameters_array(result)
â”‚   â”œâ”€ env.step(action) â†’ reward
â”‚   â””â”€ save (image, action, reward) â†’ buffer/
â””â”€ ç»“æœ: (obs, action, reward, next_obs, done) è½¨è¿¹æ•°æ®

é˜¶æ®µ3: RLè®­ç»ƒ (Python 3.10)
â”œâ”€ rlft/train.py:
â”‚   â”œâ”€ è¯»å–bufferä¸­çš„æ‰€æœ‰è½¨è¿¹
â”‚   â”œâ”€ Criticå­¦ä¹ Q(s,a) â† ä»reward
â”‚   â”œâ”€ Actorä¼˜åŒ–: max Q(s, Actor(s))
â”‚   â””â”€ ä¿å­˜æ›´æ–°åçš„Actor
â””â”€ ç»“æœ: è¶…è¶Šç›‘ç£å­¦ä¹ çš„æ€§èƒ½

é˜¶æ®µ4: å¾ªç¯æ›´æ–°
â”œâ”€ é‡å¯ftrl_serverï¼ŒåŠ è½½æœ€æ–°Actor
â””â”€ ç»§ç»­æ”¶é›†æ•°æ® â†’ è®­ç»ƒ â†’ æ›´æ–°...
```

### ç¯å¢ƒéš”ç¦»ï¼šä¸ºä»€ä¹ˆéœ€è¦HTTPï¼Ÿ

```
é—®é¢˜: VLM+DPTéœ€è¦Python 3.10ï¼Œä½†ROSåªæ”¯æŒPython 3.8

è§£å†³:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ evaluate_ftrl_single.py         â”‚
â”‚ (Python 3.8 + ROS)              â”‚
â”‚ â”œâ”€ Gymç¯å¢ƒ                      â”‚
â”‚ â”œâ”€ Gazeboä»¿çœŸ                   â”‚
â”‚ â””â”€ HTTPè°ƒç”¨ â†’                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ qwen_server.py                  â”‚
â”‚ (Python 3.10)                   â”‚
â”‚ â”œâ”€ VLM+DPT (ç›‘ç£å­¦ä¹ checkpoint) â”‚
â”‚ â””â”€ è¿”å›7ä¸ªå‚æ•°                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ³¨æ„:
- æ•°æ®æ”¶é›†é˜¶æ®µä½¿ç”¨ script/qwen/qwen_server.py
- ftrl_server.py æ˜¯ä¸ºäº†å°†æ¥è®­ç»ƒååŠ è½½æ–°Actorï¼ˆå¯é€‰ï¼‰
- å½“å‰å®ç°ï¼šç›´æ¥å¤ç”¨æˆç†Ÿçš„qwen_server.py
```

### å¥½è½¨è¿¹ vs åè½¨è¿¹

**TD3è‡ªåŠ¨å¤„ç†**ï¼š
```python
# å¥½è½¨è¿¹: rewardé«˜ â†’ Qå€¼é«˜ â†’ Actoræœè¿™ä¸ªæ–¹å‘æ›´æ–°
trajectory_good = [(s, a, +10.0, ...), ...]
â†’ VLM+DPTå­¦ä¹ "äº§ç”Ÿè¿™äº›action"

# åè½¨è¿¹: rewardä½ â†’ Qå€¼ä½ â†’ Actorè¿œç¦»è¿™ä¸ªæ–¹å‘
trajectory_bad = [(s, a, -10.0, ...), ...]
â†’ VLM+DPTå­¦ä¹ "é¿å…è¿™äº›action"

# ä¸éœ€è¦æ‰‹åŠ¨è¿‡æ»¤ï¼Criticä¼šè‡ªåŠ¨ç»™åè½¨è¿¹ä½Qå€¼
```

**å¯é€‰å¢å¼º**ï¼š
- Reward Shaping: ç²¾ç»†åŒ–å¥–åŠ±ä¿¡å·
- Prioritized Replay: ä¼˜å…ˆå­¦ä¹ TD errorå¤§çš„æ ·æœ¬
- ä½†é»˜è®¤TD3æœºåˆ¶å·²ç»è¶³å¤Ÿå¥½ï¼

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [RLFTæŠ€æœ¯æ–‡æ¡£](../../rlft/README.md) - è¯¦ç»†å®ç°è¯´æ˜
- [APPLRè®ºæ–‡](../../../applr.pdf) - Baselineæ–¹æ³•
- [CLAUDE.md](../../../CLAUDE.md) - é¡¹ç›®æ€»è§ˆ

---

**Questions? è”ç³»é¡¹ç›®ç»´æŠ¤è€…**
